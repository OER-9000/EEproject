{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchを使う\n",
    "PyTorchは深層学習フレームワークの1つ。1からプログラムをするのに比べ簡単で、かつGPUを利用した計算ができる。ほかにもTensorFlow, Chainerなど多くのフレームワークがある。本格的なディープラーニングでは、事実上、このいずれかを利用することになる。\n",
    "\n",
    "使うときには以下のようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多クラスロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは、変数は Tensor という型に入れる。numpyのデータからはfrom_numpy関数で変換できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "M = iris.target_names.size # クラス(アヤメの種類)の数\n",
    "D = iris.data.shape[1] # 特徴量の数\n",
    "X = torch.from_numpy(iris.data).float()\n",
    "C = torch.from_numpy(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0::2] # 偶数番目\n",
    "X_test = X[1::2] # 奇数番目\n",
    "C_train = C[0::2]\n",
    "C_test = C[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラスロジスティック回帰は、\n",
    "<span id=\"multiclasslogistic\">\n",
    "\\begin{align*}\n",
    "z_m&=\\boldsymbol{w_m}^\\top\\boldsymbol{x} \\\\\n",
    "y_m&=\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}\n",
    "\\end{align*}\n",
    "</span>\n",
    "だった。まず $\\boldsymbol{x}$ を $\\boldsymbol{W}$ で線形変換して $\\boldsymbol{z}$ を得る。次に $\\boldsymbol{z}$ を softmax 関数に通して $\\boldsymbol{y}$ を得るという手順であった。\n",
    "\n",
    "PyTorchでは、このようなベクトル(テンソル)の変換の繰り返しでモデルを定義する。線形変換には Linear, 対数 softmax には LogSoftmax というモジュールをモデルに順番に追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear\", torch.nn.Linear(in_features=D,out_features=M))\n",
    "model.add_module(\"softmax\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は、自分で定義しなくとも最初からいろいろ用意されている。ロジスティック回帰の損失関数は負の対数尤度 (Negative Log Likelihood) なので NLLLoss という関数が使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossEntropyLoss = torch.nn.NLLLoss() # 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchを使ったモデル学習の基本は次の通りである。\n",
    "\n",
    "まず、forward (前向き) 計算を行う。入力から出力への向きのことを前向きと呼ぶ。ここでは $\\boldsymbol{x}$ から $\\boldsymbol{z}$ 、$\\boldsymbol{z}$ から $\\boldsymbol{y}$ の計算である。上で定義した model 関数を使うと、出力の計算(=forward計算)が行える。\n",
    "\n",
    "出力がわかると、教師データとの関係から損失が計算できる。次に損失を計算する。上で定義した crossEntropyLoss 関数に、出力と教師データを与えると計算してくれる。\n",
    "\n",
    "次に、最小化したい値(ここでは loss)に対してbackward (後向き) 計算を行う。ここで計算されているのは誤差と呼ばれる値である。多クラスロジスティック回帰の場合には、クラス $m$ の教師信号との誤差は $y_m-t_m$ である。第2回の課題(2)でやったように、 $E$ の勾配は\n",
    "$\\sum_k (y_{km}-t_{km})\\boldsymbol{x}$ となる。つまり、誤差と入力の積によって勾配が計算できる。PyTorch の backward 関数は、各層の誤差を計算するのと同時に、その層への入力との積から勾配 grad を計算し、パラメータの中に保存する。なお計算した勾配は蓄積される仕様なので、backward計算の前にゼロクリアしている。\n",
    "\n",
    "後は今までと同じように、勾配に学習係数をかけた値をパラメータから引けば1回の更新が終了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "step_num = 10000\n",
    "\n",
    "for i in range(step_num):\n",
    "    Y = model(X_train) # forward計算\n",
    "    loss = crossEntropyLoss(Y, C_train) # 損失の計算\n",
    "    W, b = model.parameters()\n",
    "    if W.grad is not None:\n",
    "        W.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "\n",
    "    loss.backward() # backward計算\n",
    "    # 勾配法\n",
    "    W.data -= learning_rate * W.grad.data\n",
    "    b.data -= learning_rate * b.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の済んだモデルにテストデータの入力を入れると各クラスの対数確率が得られる。対数確率が最大のクラスが認識結果となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model,X,C):\n",
    "    Y = model(X)\n",
    "    result = np.array([np.argmax(y) for y in Y.detach().numpy()])\n",
    "    answer = C.detach().numpy()\n",
    "    return np.sum(np.equal(result, answer)) / C.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,X_test,C_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手書き数字の認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アヤメの分類よりももう少しだけ本格的な問題を解いてみる。[MNIST](https://en.wikipedia.org/wiki/MNIST_database)は0から9までの手書き数字画像のデータベースである。これを使って、多クラスロジスティック回帰モデルで手書き数字の認識をやってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# プロキシ設定 (大学の中でやるときだけ)\n",
    "proxy = urllib.request.ProxyHandler({'http': 'http://proxya.cc.utsunomiya-u.ac.jp:8080',\n",
    "                                     'https': 'http://proxya.cc.utsunomiya-u.ac.jp:8080'})\n",
    "opener = urllib.request.build_opener(proxy)\n",
    "urllib.request.install_opener(opener)\n",
    "# プロキシ設定 ここまで\n",
    "\n",
    "if not os.path.exists(\"mldata\"):\n",
    "    os.makedirs(\"mldata\")\n",
    "mnist_save_path = os.path.join(\"mldata\",\"mnist-original.mat\")\n",
    "if not os.path.exists(mnist_save_path):\n",
    "    mnist_url = urllib.request.urlopen(\"http://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\")\n",
    "    with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "        copyfileobj(mnist_url, matlab_file)\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\",data_home=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist.data には0から255までの画素値、 mnist.target にはどの数字かを表す数値が入っている。まずXを最大値1に正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mnist.data / 255.0\n",
    "C = mnist.target.astype(np.long)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数は全部で70000個、特徴量の数は画素数($28\\times28=784$)である。\n",
    "\n",
    "以下のコードで、データの1つを描画できる。Xの添字を適当に変えて、MNISTがどのようなデータなのか眺めておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a40515668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[23450].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # 数字の数\n",
    "D = 28 * 28 # 特徴量の数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90%のデータで訓練、残り10%でテストする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, C_train, C_test = train_test_split(X, C, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル定義はirisの時と同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear\", torch.nn.Linear(in_features=D,out_features=M))\n",
    "model.add_module(\"softmax\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習部分については関数化しておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, C, learning_rate=0.5, step_num=100):\n",
    "    for i in range(step_num):\n",
    "        Y = model(X) # forward計算\n",
    "        loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "        W, b = model.parameters()\n",
    "        if W.grad is not None:\n",
    "            W.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "\n",
    "        loss.backward() # backward計算\n",
    "        # 勾配法\n",
    "        W.data -= learning_rate * W.grad.data\n",
    "        b.data -= learning_rate * b.grad.data\n",
    "        print(\"epoch {}: loss = {}\".format(i+1,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では学習してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 2.2858643531799316\n",
      "epoch 2: loss = 1.8229169845581055\n",
      "epoch 3: loss = 1.5158662796020508\n",
      "epoch 4: loss = 1.3045480251312256\n",
      "epoch 5: loss = 1.1576627492904663\n",
      "epoch 6: loss = 1.0474191904067993\n",
      "epoch 7: loss = 0.965282678604126\n",
      "epoch 8: loss = 0.9022248387336731\n",
      "epoch 9: loss = 0.8522015810012817\n",
      "epoch 10: loss = 0.8114147782325745\n",
      "epoch 11: loss = 0.7773453593254089\n",
      "epoch 12: loss = 0.7483292818069458\n",
      "epoch 13: loss = 0.7232434153556824\n",
      "epoch 14: loss = 0.7013031244277954\n",
      "epoch 15: loss = 0.681917667388916\n",
      "epoch 16: loss = 0.6646405458450317\n",
      "epoch 17: loss = 0.6491319537162781\n",
      "epoch 18: loss = 0.6351189017295837\n",
      "epoch 19: loss = 0.6223824620246887\n",
      "epoch 20: loss = 0.6107408404350281\n",
      "epoch 21: loss = 0.6000545620918274\n",
      "epoch 22: loss = 0.5902044773101807\n",
      "epoch 23: loss = 0.5810917615890503\n",
      "epoch 24: loss = 0.5726215839385986\n",
      "epoch 25: loss = 0.5647355914115906\n",
      "epoch 26: loss = 0.5573670864105225\n",
      "epoch 27: loss = 0.5504589676856995\n",
      "epoch 28: loss = 0.5439701676368713\n",
      "epoch 29: loss = 0.537858247756958\n",
      "epoch 30: loss = 0.532096266746521\n",
      "epoch 31: loss = 0.5266414284706116\n",
      "epoch 32: loss = 0.5214780569076538\n",
      "epoch 33: loss = 0.5165776014328003\n",
      "epoch 34: loss = 0.5119146108627319\n",
      "epoch 35: loss = 0.5074751973152161\n",
      "epoch 36: loss = 0.5032467842102051\n",
      "epoch 37: loss = 0.4992064833641052\n",
      "epoch 38: loss = 0.495342880487442\n",
      "epoch 39: loss = 0.4916451871395111\n",
      "epoch 40: loss = 0.4880969226360321\n",
      "epoch 41: loss = 0.4846959114074707\n",
      "epoch 42: loss = 0.4814252555370331\n",
      "epoch 43: loss = 0.4782868027687073\n",
      "epoch 44: loss = 0.4752567708492279\n",
      "epoch 45: loss = 0.47234228253364563\n",
      "epoch 46: loss = 0.4695339798927307\n",
      "epoch 47: loss = 0.4668201208114624\n",
      "epoch 48: loss = 0.46420037746429443\n",
      "epoch 49: loss = 0.46166765689849854\n",
      "epoch 50: loss = 0.4592188596725464\n",
      "epoch 51: loss = 0.4568468928337097\n",
      "epoch 52: loss = 0.4545528292655945\n",
      "epoch 53: loss = 0.45232295989990234\n",
      "epoch 54: loss = 0.450164258480072\n",
      "epoch 55: loss = 0.4480716288089752\n",
      "epoch 56: loss = 0.44603899121284485\n",
      "epoch 57: loss = 0.4440551698207855\n",
      "epoch 58: loss = 0.4421355128288269\n",
      "epoch 59: loss = 0.4402660131454468\n",
      "epoch 60: loss = 0.4384475350379944\n",
      "epoch 61: loss = 0.4366786777973175\n",
      "epoch 62: loss = 0.43495312333106995\n",
      "epoch 63: loss = 0.4332706332206726\n",
      "epoch 64: loss = 0.4316353499889374\n",
      "epoch 65: loss = 0.4300326108932495\n",
      "epoch 66: loss = 0.428475946187973\n",
      "epoch 67: loss = 0.426951140165329\n",
      "epoch 68: loss = 0.42546558380126953\n",
      "epoch 69: loss = 0.4240139126777649\n",
      "epoch 70: loss = 0.42259538173675537\n",
      "epoch 71: loss = 0.4212097227573395\n",
      "epoch 72: loss = 0.4198558032512665\n",
      "epoch 73: loss = 0.41852471232414246\n",
      "epoch 74: loss = 0.41723066568374634\n",
      "epoch 75: loss = 0.41595831513404846\n",
      "epoch 76: loss = 0.4147137999534607\n",
      "epoch 77: loss = 0.41349512338638306\n",
      "epoch 78: loss = 0.4123014509677887\n",
      "epoch 79: loss = 0.411131352186203\n",
      "epoch 80: loss = 0.40998801589012146\n",
      "epoch 81: loss = 0.40886011719703674\n",
      "epoch 82: loss = 0.4077570140361786\n",
      "epoch 83: loss = 0.4066799283027649\n",
      "epoch 84: loss = 0.40561819076538086\n",
      "epoch 85: loss = 0.4045744836330414\n",
      "epoch 86: loss = 0.40354880690574646\n",
      "epoch 87: loss = 0.40254735946655273\n",
      "epoch 88: loss = 0.40156352519989014\n",
      "epoch 89: loss = 0.40059375762939453\n",
      "epoch 90: loss = 0.3996411859989166\n",
      "epoch 91: loss = 0.39870643615722656\n",
      "epoch 92: loss = 0.39778420329093933\n",
      "epoch 93: loss = 0.39688411355018616\n",
      "epoch 94: loss = 0.3959912061691284\n",
      "epoch 95: loss = 0.3951204717159271\n",
      "epoch 96: loss = 0.3942584693431854\n",
      "epoch 97: loss = 0.39341139793395996\n",
      "epoch 98: loss = 0.39258041977882385\n",
      "epoch 99: loss = 0.3917616009712219\n",
      "epoch 100: loss = 0.3909529149532318\n",
      "epoch 101: loss = 0.39015865325927734\n",
      "epoch 102: loss = 0.38937950134277344\n",
      "epoch 103: loss = 0.3886108994483948\n",
      "epoch 104: loss = 0.38785025477409363\n",
      "epoch 105: loss = 0.3871005177497864\n",
      "epoch 106: loss = 0.3863643705844879\n",
      "epoch 107: loss = 0.38563886284828186\n",
      "epoch 108: loss = 0.38492462038993835\n",
      "epoch 109: loss = 0.38421934843063354\n",
      "epoch 110: loss = 0.3835211992263794\n",
      "epoch 111: loss = 0.38283589482307434\n",
      "epoch 112: loss = 0.38216137886047363\n",
      "epoch 113: loss = 0.3814938962459564\n",
      "epoch 114: loss = 0.38083788752555847\n",
      "epoch 115: loss = 0.3801867365837097\n",
      "epoch 116: loss = 0.37954455614089966\n",
      "epoch 117: loss = 0.37891685962677\n",
      "epoch 118: loss = 0.3782908618450165\n",
      "epoch 119: loss = 0.37767457962036133\n",
      "epoch 120: loss = 0.3770671486854553\n",
      "epoch 121: loss = 0.37646758556365967\n",
      "epoch 122: loss = 0.37587589025497437\n",
      "epoch 123: loss = 0.37529104948043823\n",
      "epoch 124: loss = 0.37471166253089905\n",
      "epoch 125: loss = 0.37414249777793884\n",
      "epoch 126: loss = 0.3735780715942383\n",
      "epoch 127: loss = 0.37302470207214355\n",
      "epoch 128: loss = 0.3724718391895294\n",
      "epoch 129: loss = 0.3719289004802704\n",
      "epoch 130: loss = 0.3713909387588501\n",
      "epoch 131: loss = 0.3708578944206238\n",
      "epoch 132: loss = 0.37033483386039734\n",
      "epoch 133: loss = 0.3698155879974365\n",
      "epoch 134: loss = 0.3693069517612457\n",
      "epoch 135: loss = 0.3687964379787445\n",
      "epoch 136: loss = 0.36829596757888794\n",
      "epoch 137: loss = 0.36780256032943726\n",
      "epoch 138: loss = 0.3673118054866791\n",
      "epoch 139: loss = 0.3668256402015686\n",
      "epoch 140: loss = 0.36634695529937744\n",
      "epoch 141: loss = 0.3658720552921295\n",
      "epoch 142: loss = 0.3654032051563263\n",
      "epoch 143: loss = 0.3649377226829529\n",
      "epoch 144: loss = 0.36448198556900024\n",
      "epoch 145: loss = 0.364025354385376\n",
      "epoch 146: loss = 0.36357587575912476\n",
      "epoch 147: loss = 0.3631335496902466\n",
      "epoch 148: loss = 0.36269041895866394\n",
      "epoch 149: loss = 0.36225688457489014\n",
      "epoch 150: loss = 0.3618239164352417\n",
      "epoch 151: loss = 0.3613966703414917\n",
      "epoch 152: loss = 0.36097145080566406\n",
      "epoch 153: loss = 0.36055707931518555\n",
      "epoch 154: loss = 0.3601413071155548\n",
      "epoch 155: loss = 0.3597293496131897\n",
      "epoch 156: loss = 0.359323650598526\n",
      "epoch 157: loss = 0.35892269015312195\n",
      "epoch 158: loss = 0.35852572321891785\n",
      "epoch 159: loss = 0.35812970995903015\n",
      "epoch 160: loss = 0.35773763060569763\n",
      "epoch 161: loss = 0.3573501408100128\n",
      "epoch 162: loss = 0.3569650650024414\n",
      "epoch 163: loss = 0.35658320784568787\n",
      "epoch 164: loss = 0.3562091290950775\n",
      "epoch 165: loss = 0.3558332026004791\n",
      "epoch 166: loss = 0.35546737909317017\n",
      "epoch 167: loss = 0.35509878396987915\n",
      "epoch 168: loss = 0.3547363579273224\n",
      "epoch 169: loss = 0.3543778359889984\n",
      "epoch 170: loss = 0.35402196645736694\n",
      "epoch 171: loss = 0.35366687178611755\n",
      "epoch 172: loss = 0.3533160388469696\n",
      "epoch 173: loss = 0.35297030210494995\n",
      "epoch 174: loss = 0.3526269495487213\n",
      "epoch 175: loss = 0.3522832989692688\n",
      "epoch 176: loss = 0.35194477438926697\n",
      "epoch 177: loss = 0.35161080956459045\n",
      "epoch 178: loss = 0.3512785732746124\n",
      "epoch 179: loss = 0.3509484529495239\n",
      "epoch 180: loss = 0.35062333941459656\n",
      "epoch 181: loss = 0.3502979576587677\n",
      "epoch 182: loss = 0.34997549653053284\n",
      "epoch 183: loss = 0.3496546447277069\n",
      "epoch 184: loss = 0.349336713552475\n",
      "epoch 185: loss = 0.34902310371398926\n",
      "epoch 186: loss = 0.34871023893356323\n",
      "epoch 187: loss = 0.3484017848968506\n",
      "epoch 188: loss = 0.3480992913246155\n",
      "epoch 189: loss = 0.3477950096130371\n",
      "epoch 190: loss = 0.34749266505241394\n",
      "epoch 191: loss = 0.3471943438053131\n",
      "epoch 192: loss = 0.3468988537788391\n",
      "epoch 193: loss = 0.34660446643829346\n",
      "epoch 194: loss = 0.3463110327720642\n",
      "epoch 195: loss = 0.34602341055870056\n",
      "epoch 196: loss = 0.34573352336883545\n",
      "epoch 197: loss = 0.3454475700855255\n",
      "epoch 198: loss = 0.3451649248600006\n",
      "epoch 199: loss = 0.3448832631111145\n",
      "epoch 200: loss = 0.3446035087108612\n"
     ]
    }
   ],
   "source": [
    "X_train_t=torch.from_numpy(X_train).float()\n",
    "C_train_t=torch.from_numpy(C_train)\n",
    "train(model,X_train_t,C_train_t,step_num=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率を調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035714285714286"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_t=torch.from_numpy(X_test).float()\n",
    "C_test_t=torch.from_numpy(C_test)\n",
    "accuracy(model,X_test_t,C_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習\n",
    "ここまでは、全データで一挙に重みを更新した。これをバッチ学習という。これに対し、深層学習の世界では、全データをいくつかのミニバッチに分割して勾配降下法を実行するSGD(確率的勾配降下法)、またの名をミニバッチ学習という学習方法がよく使われる。この方法は、収束が速く、局所最小値にはまる危険が少ない特長を持っている。\n",
    "\n",
    "PyTorchでは、DataLoaderという機能を使うとデータを自動的にミニバッチに分割してくれる。この機能を使うために、まずMNIST用にデータセットを定義しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MNISTTrainDataSet(Dataset):\n",
    "    def __len__(self):\n",
    "        return X_train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X_train[idx].astype('float'), C_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のDataLoaderでは、バッチサイズを50とする。つまり、70000個のデータを50個ずつに分割し、それぞれで勾配降下法によりパラメータを更新する。これを全データに対して行う。全データを1回なめることを1エポックという。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTTrainDataSet()\n",
    "train_loader = DataLoader(dataset=mnist,batch_size=50,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルは上と同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear\", torch.nn.Linear(in_features=D,out_features=M))\n",
    "model.add_module(\"softmax\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習部分は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate=0.5, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, C in train_loader:\n",
    "            X = X.float() # DataLoaderはデータをdouble型で渡すので、後の処理のためにfloat型に変換\n",
    "            Y = model(X) # forward計算\n",
    "            loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "            W, b = model.parameters()\n",
    "            if W.grad is not None:\n",
    "                W.grad.data.zero_()\n",
    "                b.grad.data.zero_()\n",
    "\n",
    "            loss.backward() # backward計算\n",
    "            # 勾配法\n",
    "            W.data -= learning_rate * W.grad.data\n",
    "            b.data -= learning_rate * b.grad.data\n",
    "        Y = model(X_train_t)\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, crossEntropyLoss(Y, C_train_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3389\n",
      "Epoch [2/100], Loss: 0.3083\n",
      "Epoch [3/100], Loss: 0.2926\n",
      "Epoch [4/100], Loss: 0.2854\n",
      "Epoch [5/100], Loss: 0.2796\n",
      "Epoch [6/100], Loss: 0.2781\n",
      "Epoch [7/100], Loss: 0.2720\n",
      "Epoch [8/100], Loss: 0.2689\n",
      "Epoch [9/100], Loss: 0.2658\n",
      "Epoch [10/100], Loss: 0.2626\n",
      "Epoch [11/100], Loss: 0.2647\n",
      "Epoch [12/100], Loss: 0.2610\n",
      "Epoch [13/100], Loss: 0.2603\n",
      "Epoch [14/100], Loss: 0.2584\n",
      "Epoch [15/100], Loss: 0.2561\n",
      "Epoch [16/100], Loss: 0.2563\n",
      "Epoch [17/100], Loss: 0.2552\n",
      "Epoch [18/100], Loss: 0.2540\n",
      "Epoch [19/100], Loss: 0.2545\n",
      "Epoch [20/100], Loss: 0.2526\n",
      "Epoch [21/100], Loss: 0.2527\n",
      "Epoch [22/100], Loss: 0.2507\n",
      "Epoch [23/100], Loss: 0.2518\n",
      "Epoch [24/100], Loss: 0.2516\n",
      "Epoch [25/100], Loss: 0.2505\n",
      "Epoch [26/100], Loss: 0.2500\n",
      "Epoch [27/100], Loss: 0.2483\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2481\n",
      "Epoch [30/100], Loss: 0.2479\n",
      "Epoch [31/100], Loss: 0.2466\n",
      "Epoch [32/100], Loss: 0.2453\n",
      "Epoch [33/100], Loss: 0.2484\n",
      "Epoch [34/100], Loss: 0.2443\n",
      "Epoch [35/100], Loss: 0.2460\n",
      "Epoch [36/100], Loss: 0.2455\n",
      "Epoch [37/100], Loss: 0.2428\n",
      "Epoch [38/100], Loss: 0.2426\n",
      "Epoch [39/100], Loss: 0.2419\n",
      "Epoch [40/100], Loss: 0.2430\n",
      "Epoch [41/100], Loss: 0.2420\n",
      "Epoch [42/100], Loss: 0.2420\n",
      "Epoch [43/100], Loss: 0.2408\n",
      "Epoch [44/100], Loss: 0.2425\n",
      "Epoch [45/100], Loss: 0.2426\n",
      "Epoch [46/100], Loss: 0.2406\n",
      "Epoch [47/100], Loss: 0.2404\n",
      "Epoch [48/100], Loss: 0.2424\n",
      "Epoch [49/100], Loss: 0.2405\n",
      "Epoch [50/100], Loss: 0.2389\n",
      "Epoch [51/100], Loss: 0.2392\n",
      "Epoch [52/100], Loss: 0.2380\n",
      "Epoch [53/100], Loss: 0.2382\n",
      "Epoch [54/100], Loss: 0.2386\n",
      "Epoch [55/100], Loss: 0.2386\n",
      "Epoch [56/100], Loss: 0.2410\n",
      "Epoch [57/100], Loss: 0.2383\n",
      "Epoch [58/100], Loss: 0.2374\n",
      "Epoch [59/100], Loss: 0.2380\n",
      "Epoch [60/100], Loss: 0.2380\n",
      "Epoch [61/100], Loss: 0.2387\n",
      "Epoch [62/100], Loss: 0.2390\n",
      "Epoch [63/100], Loss: 0.2378\n",
      "Epoch [64/100], Loss: 0.2370\n",
      "Epoch [65/100], Loss: 0.2359\n",
      "Epoch [66/100], Loss: 0.2394\n",
      "Epoch [67/100], Loss: 0.2376\n",
      "Epoch [68/100], Loss: 0.2357\n",
      "Epoch [69/100], Loss: 0.2373\n",
      "Epoch [70/100], Loss: 0.2353\n",
      "Epoch [71/100], Loss: 0.2354\n",
      "Epoch [72/100], Loss: 0.2364\n",
      "Epoch [73/100], Loss: 0.2365\n",
      "Epoch [74/100], Loss: 0.2353\n",
      "Epoch [75/100], Loss: 0.2360\n",
      "Epoch [76/100], Loss: 0.2341\n",
      "Epoch [77/100], Loss: 0.2352\n",
      "Epoch [78/100], Loss: 0.2347\n",
      "Epoch [79/100], Loss: 0.2359\n",
      "Epoch [80/100], Loss: 0.2392\n",
      "Epoch [81/100], Loss: 0.2344\n",
      "Epoch [82/100], Loss: 0.2362\n",
      "Epoch [83/100], Loss: 0.2334\n",
      "Epoch [84/100], Loss: 0.2337\n",
      "Epoch [85/100], Loss: 0.2353\n",
      "Epoch [86/100], Loss: 0.2345\n",
      "Epoch [87/100], Loss: 0.2339\n",
      "Epoch [88/100], Loss: 0.2346\n",
      "Epoch [89/100], Loss: 0.2331\n",
      "Epoch [90/100], Loss: 0.2326\n",
      "Epoch [91/100], Loss: 0.2330\n",
      "Epoch [92/100], Loss: 0.2347\n",
      "Epoch [93/100], Loss: 0.2334\n",
      "Epoch [94/100], Loss: 0.2327\n",
      "Epoch [95/100], Loss: 0.2356\n",
      "Epoch [96/100], Loss: 0.2317\n",
      "Epoch [97/100], Loss: 0.2312\n",
      "Epoch [98/100], Loss: 0.2316\n",
      "Epoch [99/100], Loss: 0.2347\n",
      "Epoch [100/100], Loss: 0.2325\n"
     ]
    }
   ],
   "source": [
    "train(model,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9231428571428572"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,X_test_t,C_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forwardネットワーク\n",
    "多クラスロジスティック回帰を少し拡張してみる。(記号を少し変えてある)\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_1&={\\boldsymbol{W}^{(1)}}^\\top\\boldsymbol{x} \\\\\n",
    "\\boldsymbol{z}_1&=\\sigma(\\boldsymbol{a}_1) \\\\\n",
    "\\boldsymbol{a}_2&={\\boldsymbol{W}^{(2)}}^\\top\\boldsymbol{z}_1^+ \\\\\n",
    "\\boldsymbol{y}&=\\text{softmax}(\\boldsymbol{a}_2)\n",
    "\\end{align*}\n",
    "$\\boldsymbol{x}$ → $\\boldsymbol{z_1}$ → $\\boldsymbol{y}$ の順でデータが変換されているのがわかるだろうか。\n",
    "\n",
    "$\\boldsymbol{W}^{(1)}$ は $(D+1)\\times H$行列、 $\\boldsymbol{W}^{(2)}$ は $(H+1)\\times M$行列である。(プラス1は切片のため)  $\\boldsymbol{z}^+$ は $\\boldsymbol{z}$ を1で拡張したベクトルを表す。$H$ は任意に決められるが、この数が重要であることが後でわかる。\n",
    "\n",
    "$\\sigma(\\cdot)$ はロジスティック関数であり、上式の $\\boldsymbol{z}_1=\\sigma(\\boldsymbol{a}_1)$ とは $\\boldsymbol{a}_1$ の各次元の値をロジスティック関数に通した値をつなげてベクトルにするという意味である。この関数は、PyTorchでは torch.nn.Sigmoid() として提供されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate=0.5, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, C in train_loader:\n",
    "            X = X.float() # DataLoaderはデータをdouble型で渡すので、後の処理のためにfloat型に変換\n",
    "            Y = model(X) # forward計算\n",
    "            loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "            W1, b1, W2, b2 = model.parameters()\n",
    "            if W1.grad is not None:\n",
    "                W1.grad.data.zero_()\n",
    "                b1.grad.data.zero_()\n",
    "            if W2.grad is not None:\n",
    "                W2.grad.data.zero_()\n",
    "                b2.grad.data.zero_()\n",
    "            loss.backward() # backward計算\n",
    "            # 勾配法\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            b1.data -= learning_rate * b1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "            b2.data -= learning_rate * b2.grad.data\n",
    "\n",
    "        Y = model(X_train_t)\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, crossEntropyLoss(Y, C_train_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 500\n",
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear1\", torch.nn.Linear(in_features=D,out_features=hidden_units))\n",
    "model.add_module(\"sigmoid\", torch.nn.Sigmoid())\n",
    "model.add_module(\"linear2\", torch.nn.Linear(in_features=hidden_units,out_features=M))\n",
    "model.add_module(\"softmax2\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2503\n",
      "Epoch [2/100], Loss: 0.1768\n",
      "Epoch [3/100], Loss: 0.1440\n",
      "Epoch [4/100], Loss: 0.1107\n",
      "Epoch [5/100], Loss: 0.0878\n",
      "Epoch [6/100], Loss: 0.0769\n",
      "Epoch [7/100], Loss: 0.0660\n",
      "Epoch [8/100], Loss: 0.0590\n",
      "Epoch [9/100], Loss: 0.0498\n",
      "Epoch [10/100], Loss: 0.0461\n",
      "Epoch [11/100], Loss: 0.0416\n",
      "Epoch [12/100], Loss: 0.0370\n",
      "Epoch [13/100], Loss: 0.0323\n",
      "Epoch [14/100], Loss: 0.0305\n",
      "Epoch [15/100], Loss: 0.0268\n",
      "Epoch [16/100], Loss: 0.0251\n",
      "Epoch [17/100], Loss: 0.0215\n",
      "Epoch [18/100], Loss: 0.0190\n",
      "Epoch [19/100], Loss: 0.0171\n",
      "Epoch [20/100], Loss: 0.0159\n",
      "Epoch [21/100], Loss: 0.0165\n",
      "Epoch [22/100], Loss: 0.0135\n",
      "Epoch [23/100], Loss: 0.0126\n",
      "Epoch [24/100], Loss: 0.0116\n",
      "Epoch [25/100], Loss: 0.0116\n",
      "Epoch [26/100], Loss: 0.0104\n",
      "Epoch [27/100], Loss: 0.0093\n",
      "Epoch [28/100], Loss: 0.0088\n",
      "Epoch [29/100], Loss: 0.0080\n",
      "Epoch [30/100], Loss: 0.0080\n",
      "Epoch [31/100], Loss: 0.0070\n",
      "Epoch [32/100], Loss: 0.0067\n",
      "Epoch [33/100], Loss: 0.0061\n",
      "Epoch [34/100], Loss: 0.0056\n",
      "Epoch [35/100], Loss: 0.0055\n",
      "Epoch [36/100], Loss: 0.0052\n",
      "Epoch [37/100], Loss: 0.0051\n",
      "Epoch [38/100], Loss: 0.0045\n",
      "Epoch [39/100], Loss: 0.0045\n",
      "Epoch [40/100], Loss: 0.0043\n",
      "Epoch [41/100], Loss: 0.0041\n",
      "Epoch [42/100], Loss: 0.0039\n",
      "Epoch [43/100], Loss: 0.0037\n",
      "Epoch [44/100], Loss: 0.0035\n",
      "Epoch [45/100], Loss: 0.0033\n",
      "Epoch [46/100], Loss: 0.0033\n",
      "Epoch [47/100], Loss: 0.0031\n",
      "Epoch [48/100], Loss: 0.0030\n",
      "Epoch [49/100], Loss: 0.0031\n",
      "Epoch [50/100], Loss: 0.0029\n",
      "Epoch [51/100], Loss: 0.0027\n",
      "Epoch [52/100], Loss: 0.0026\n",
      "Epoch [53/100], Loss: 0.0025\n",
      "Epoch [54/100], Loss: 0.0024\n",
      "Epoch [55/100], Loss: 0.0024\n",
      "Epoch [56/100], Loss: 0.0023\n",
      "Epoch [57/100], Loss: 0.0022\n",
      "Epoch [58/100], Loss: 0.0022\n",
      "Epoch [59/100], Loss: 0.0022\n",
      "Epoch [60/100], Loss: 0.0021\n",
      "Epoch [61/100], Loss: 0.0020\n",
      "Epoch [62/100], Loss: 0.0020\n",
      "Epoch [63/100], Loss: 0.0019\n",
      "Epoch [64/100], Loss: 0.0019\n",
      "Epoch [65/100], Loss: 0.0018\n",
      "Epoch [66/100], Loss: 0.0018\n",
      "Epoch [67/100], Loss: 0.0017\n",
      "Epoch [68/100], Loss: 0.0017\n",
      "Epoch [69/100], Loss: 0.0017\n",
      "Epoch [70/100], Loss: 0.0016\n",
      "Epoch [71/100], Loss: 0.0016\n",
      "Epoch [72/100], Loss: 0.0016\n",
      "Epoch [73/100], Loss: 0.0015\n",
      "Epoch [74/100], Loss: 0.0015\n",
      "Epoch [75/100], Loss: 0.0015\n",
      "Epoch [76/100], Loss: 0.0014\n",
      "Epoch [77/100], Loss: 0.0014\n",
      "Epoch [78/100], Loss: 0.0014\n",
      "Epoch [79/100], Loss: 0.0013\n",
      "Epoch [80/100], Loss: 0.0013\n",
      "Epoch [81/100], Loss: 0.0013\n",
      "Epoch [82/100], Loss: 0.0013\n",
      "Epoch [83/100], Loss: 0.0012\n",
      "Epoch [84/100], Loss: 0.0012\n",
      "Epoch [85/100], Loss: 0.0012\n",
      "Epoch [86/100], Loss: 0.0012\n",
      "Epoch [87/100], Loss: 0.0012\n",
      "Epoch [88/100], Loss: 0.0011\n",
      "Epoch [89/100], Loss: 0.0011\n",
      "Epoch [90/100], Loss: 0.0011\n",
      "Epoch [91/100], Loss: 0.0011\n",
      "Epoch [92/100], Loss: 0.0011\n",
      "Epoch [93/100], Loss: 0.0011\n",
      "Epoch [94/100], Loss: 0.0010\n",
      "Epoch [95/100], Loss: 0.0010\n",
      "Epoch [96/100], Loss: 0.0010\n",
      "Epoch [97/100], Loss: 0.0010\n",
      "Epoch [98/100], Loss: 0.0010\n",
      "Epoch [99/100], Loss: 0.0010\n",
      "Epoch [100/100], Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "train(model,learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824285714285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,X_test_t,C_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題\n",
    "1. Feed-forwardネットワークは、「多層パーセプトロン」という別名を持つ、もっとも基本的なニューラルネットである。ニューラルネットとは何か、なぜニューラルネットと呼ばれるのかについて、研究室の蔵書を読んでおくこと。(「これならわかる深層学習入門」がおすすめ)\n",
    "1. (重要) Feed-forwardネットワークにより手書き数字画像のモデル化を行い、多クラスロジスティック回帰と性能を比較せよ。隠れ層ユニットの数 (${}=H$) について、いろいろ試してみよ。\n",
    "1. $\\boldsymbol{x}$ から $\\boldsymbol{a}$ へは線形変換、 $\\boldsymbol{a}$ から $\\boldsymbol{z}$ へは非線形変換になっている。もし $\\boldsymbol{a}$ から $\\boldsymbol{z}$ が線形変換であれば、このモデルは多クラスロジスティック回帰と等価になってしまう。なぜか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
