{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchを使う\n",
    "PyTorchは深層学習フレームワークの1つ。1からプログラムをするのに比べ簡単で、かつGPUを利用した計算ができる。ほかにもTensorFlow, Chainerなど多くのフレームワークがある。本格的なディープラーニングでは、事実上、このいずれかを利用することになる。\n",
    "\n",
    "使うときには以下のようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多クラスロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは、変数は Tensor という型に入れる。numpyのデータからはfrom_numpy関数で変換できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "M = iris.target_names.size # クラス(アヤメの種類)の数\n",
    "D = iris.data.shape[1] # 特徴量の数\n",
    "X = torch.from_numpy(iris.data).float()\n",
    "C = torch.from_numpy(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0::2] # 偶数番目\n",
    "X_test = X[1::2] # 奇数番目\n",
    "C_train = C[0::2]\n",
    "C_test = C[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラスロジスティック回帰は、\n",
    "<span id=\"multiclasslogistic\">\n",
    "\\begin{align*}\n",
    "z_m&=\\boldsymbol{w_m}^\\top\\boldsymbol{x} \\\\\n",
    "y_m&=\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}\n",
    "\\end{align*}\n",
    "</span>\n",
    "だった。まず $\\boldsymbol{x}$ を $\\boldsymbol{W}$ で線形変換して $\\boldsymbol{z}$ を得る。次に $\\boldsymbol{z}$ を softmax 関数に通して $\\boldsymbol{y}$ を得るという手順であった。\n",
    "\n",
    "PyTorchでは、このようなベクトル(テンソル)の変換の繰り返しでモデルを定義する。線形変換には Linear, 対数 softmax には LogSoftmax というモジュールをモデルに順番に追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear\", torch.nn.Linear(in_features=D,out_features=M))\n",
    "model.add_module(\"softmax\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は、自分で定義しなくとも最初からいろいろ用意されている。ロジスティック回帰の損失関数は負の対数尤度 (Negative Log Likelihood) なので NLLLoss という関数が使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossEntropyLoss = torch.nn.NLLLoss() # 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchを使ったモデル学習の基本は次の通りである。\n",
    "\n",
    "まず、forward (前向き) 計算を行う。入力から出力への向きのことを前向きと呼ぶ。ここでは $\\boldsymbol{x}$ から $\\boldsymbol{z}$ 、$\\boldsymbol{z}$ から $\\boldsymbol{y}$ の計算である。上で定義した model 関数を使うと、出力の計算(=forward計算)が行える。\n",
    "\n",
    "出力がわかると、教師データとの関係から損失が計算できる。次に損失を計算する。上で定義した crossEntropyLoss 関数に、出力と教師データを与えると計算してくれる。\n",
    "\n",
    "次に、最小化したい値(ここでは loss)に対してbackward (後向き) 計算を行う。ここで計算されているのは誤差と呼ばれる値である。多クラスロジスティック回帰の場合には、クラス $m$ の教師信号との誤差は $y_m-t_m$ である。第2回の課題(2)でやったように、 $E$ の勾配は\n",
    "$\\sum_k (y_{km}-t_{km})\\boldsymbol{x}$ となる。つまり、誤差と入力の積によって勾配が計算できる。PyTorch の backward 関数は、各層の誤差を計算するのと同時に、その層への入力との積から勾配 grad を計算し、パラメータの中に保存する。なお計算した勾配は蓄積される仕様なので、backward計算の前にゼロクリアしている。\n",
    "\n",
    "後は今までと同じように、勾配に学習係数をかけた値をパラメータから引けば1回の更新が終了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "step_num = 10000\n",
    "\n",
    "for i in range(step_num):\n",
    "    Y = model(X_train) # forward計算\n",
    "    loss = crossEntropyLoss(Y, C_train) # 損失の計算\n",
    "    W, b = model.parameters()\n",
    "    if W.grad is not None:\n",
    "        W.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "\n",
    "    loss.backward() # backward計算\n",
    "    # 勾配法\n",
    "    W.data -= learning_rate * W.grad.data\n",
    "    b.data -= learning_rate * b.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の済んだモデルにテストデータの入力を入れると各クラスの対数確率が得られる。対数確率が最大のクラスが認識結果となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = model(X_test)\n",
    "result = np.array([np.argmax(y) for y in Y_test.detach().numpy()])\n",
    "answer = C_test.detach().numpy()\n",
    "np.sum(np.equal(result, answer)) / C_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手書き数字の認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アヤメの分類よりももう少しだけ本格的な問題を解いてみる。[MNIST](https://en.wikipedia.org/wiki/MNIST_database)は0から9までの手書き数字画像のデータベースである。これを使って、多クラスロジスティック回帰モデルで手書き数字の認識をやってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# プロキシ設定 (大学の中でやるときだけ)\n",
    "proxy = urllib.request.ProxyHandler({'http': 'http://proxya.cc.utsunomiya-u.ac.jp:8080',\n",
    "                                     'https': 'http://proxya.cc.utsunomiya-u.ac.jp:8080'})\n",
    "opener = urllib.request.build_opener(proxy)\n",
    "urllib.request.install_opener(opener)\n",
    "# プロキシ設定 ここまで\n",
    "\n",
    "if not os.path.exists(\"mldata\"):\n",
    "    os.makedirs(\"mldata\")\n",
    "mnist_save_path = os.path.join(\"mldata\",\"mnist-original.mat\")\n",
    "if not os.path.exists(mnist_save_path):\n",
    "    mnist_url = urllib.request.urlopen(\"http://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\")\n",
    "    with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "        copyfileobj(mnist_url, matlab_file)\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\",data_home=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist.data には0から255までの画素値、 mnist.target にはどの数字かを表す数値が入っている。まずXを最大値1に正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mnist.data / 255.0\n",
    "C = mnist.target.astype(np.long)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数は全部で70000個、特徴量の数は画素数($28\\times28=784$)である。\n",
    "\n",
    "以下のコードで、データの1つを描画できる。Xの添字を適当に変えて、MNISTがどのようなデータなのか眺めておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a38319b38>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[23450].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # 数字の数\n",
    "D = 28 * 28 # 特徴量の数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90%のデータで訓練、残り10%でテストする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, C_train, C_test = train_test_split(X, C, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル定義はirisの時と同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear\", torch.nn.Linear(in_features=D,out_features=M))\n",
    "model.add_module(\"softmax\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習部分については関数化しておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, C, learning_rate=0.5, step_num=100):\n",
    "    for i in range(step_num):\n",
    "        Y = model(X) # forward計算\n",
    "        loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "        W, b = model.parameters()\n",
    "        if W.grad is not None:\n",
    "            W.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "\n",
    "        loss.backward() # backward計算\n",
    "        # 勾配法\n",
    "        W.data -= learning_rate * W.grad.data\n",
    "        b.data -= learning_rate * b.grad.data\n",
    "        print(\"epoch {}: loss = {}\".format(i+1,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では学習してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 2.3183979988098145\n",
      "epoch 2: loss = 1.8407660722732544\n",
      "epoch 3: loss = 1.5262320041656494\n",
      "epoch 4: loss = 1.3103580474853516\n",
      "epoch 5: loss = 1.1680082082748413\n",
      "epoch 6: loss = 1.0559115409851074\n",
      "epoch 7: loss = 0.9797663688659668\n",
      "epoch 8: loss = 0.9114518761634827\n",
      "epoch 9: loss = 0.8633440136909485\n",
      "epoch 10: loss = 0.8175839781761169\n",
      "epoch 11: loss = 0.783067524433136\n",
      "epoch 12: loss = 0.7512417435646057\n",
      "epoch 13: loss = 0.7253132462501526\n",
      "epoch 14: loss = 0.702219545841217\n",
      "epoch 15: loss = 0.6823620200157166\n",
      "epoch 16: loss = 0.6647487878799438\n",
      "epoch 17: loss = 0.6491159796714783\n",
      "epoch 18: loss = 0.6350514888763428\n",
      "epoch 19: loss = 0.6223106384277344\n",
      "epoch 20: loss = 0.6106955409049988\n",
      "epoch 21: loss = 0.6000427603721619\n",
      "epoch 22: loss = 0.590219259262085\n",
      "epoch 23: loss = 0.5811310410499573\n",
      "epoch 24: loss = 0.5726924538612366\n",
      "epoch 25: loss = 0.5648188591003418\n",
      "epoch 26: loss = 0.5574703216552734\n",
      "epoch 27: loss = 0.5505785346031189\n",
      "epoch 28: loss = 0.5441033244132996\n",
      "epoch 29: loss = 0.5380085706710815\n",
      "epoch 30: loss = 0.5322510600090027\n",
      "epoch 31: loss = 0.5268118381500244\n",
      "epoch 32: loss = 0.5216545462608337\n",
      "epoch 33: loss = 0.5167583227157593\n",
      "epoch 34: loss = 0.512105405330658\n",
      "epoch 35: loss = 0.5076704621315002\n",
      "epoch 36: loss = 0.5034425258636475\n",
      "epoch 37: loss = 0.4994073212146759\n",
      "epoch 38: loss = 0.4955483376979828\n",
      "epoch 39: loss = 0.49184954166412354\n",
      "epoch 40: loss = 0.48830804228782654\n",
      "epoch 41: loss = 0.4849051535129547\n",
      "epoch 42: loss = 0.4816378057003021\n",
      "epoch 43: loss = 0.47849515080451965\n",
      "epoch 44: loss = 0.4754706025123596\n",
      "epoch 45: loss = 0.4725625813007355\n",
      "epoch 46: loss = 0.4697449505329132\n",
      "epoch 47: loss = 0.4670354723930359\n",
      "epoch 48: loss = 0.4644143283367157\n",
      "epoch 49: loss = 0.46188414096832275\n",
      "epoch 50: loss = 0.4594293534755707\n",
      "epoch 51: loss = 0.45705634355545044\n",
      "epoch 52: loss = 0.45476385951042175\n",
      "epoch 53: loss = 0.45253586769104004\n",
      "epoch 54: loss = 0.4503757655620575\n",
      "epoch 55: loss = 0.4482778310775757\n",
      "epoch 56: loss = 0.44624319672584534\n",
      "epoch 57: loss = 0.4442645013332367\n",
      "epoch 58: loss = 0.44234323501586914\n",
      "epoch 59: loss = 0.4404740333557129\n",
      "epoch 60: loss = 0.43865224719047546\n",
      "epoch 61: loss = 0.43687695264816284\n",
      "epoch 62: loss = 0.4351499378681183\n",
      "epoch 63: loss = 0.43346768617630005\n",
      "epoch 64: loss = 0.43182578682899475\n",
      "epoch 65: loss = 0.4302288591861725\n",
      "epoch 66: loss = 0.4286690652370453\n",
      "epoch 67: loss = 0.4271450340747833\n",
      "epoch 68: loss = 0.4256536066532135\n",
      "epoch 69: loss = 0.42420244216918945\n",
      "epoch 70: loss = 0.4227829575538635\n",
      "epoch 71: loss = 0.4213933050632477\n",
      "epoch 72: loss = 0.42003658413887024\n",
      "epoch 73: loss = 0.4187086820602417\n",
      "epoch 74: loss = 0.41740882396698\n",
      "epoch 75: loss = 0.4161331057548523\n",
      "epoch 76: loss = 0.41488805413246155\n",
      "epoch 77: loss = 0.4136698544025421\n",
      "epoch 78: loss = 0.41247451305389404\n",
      "epoch 79: loss = 0.41130584478378296\n",
      "epoch 80: loss = 0.41015732288360596\n",
      "epoch 81: loss = 0.4090317487716675\n",
      "epoch 82: loss = 0.40792524814605713\n",
      "epoch 83: loss = 0.40684714913368225\n",
      "epoch 84: loss = 0.405784547328949\n",
      "epoch 85: loss = 0.4047379791736603\n",
      "epoch 86: loss = 0.40371426939964294\n",
      "epoch 87: loss = 0.402704119682312\n",
      "epoch 88: loss = 0.401719868183136\n",
      "epoch 89: loss = 0.40075117349624634\n",
      "epoch 90: loss = 0.3997965455055237\n",
      "epoch 91: loss = 0.3988579511642456\n",
      "epoch 92: loss = 0.3979405462741852\n",
      "epoch 93: loss = 0.3970356583595276\n",
      "epoch 94: loss = 0.3961467146873474\n",
      "epoch 95: loss = 0.3952687680721283\n",
      "epoch 96: loss = 0.3944112956523895\n",
      "epoch 97: loss = 0.3935622274875641\n",
      "epoch 98: loss = 0.3927293121814728\n",
      "epoch 99: loss = 0.3919092118740082\n",
      "epoch 100: loss = 0.39109906554222107\n",
      "epoch 101: loss = 0.3903025984764099\n",
      "epoch 102: loss = 0.3895202577114105\n",
      "epoch 103: loss = 0.3887483775615692\n",
      "epoch 104: loss = 0.38798779249191284\n",
      "epoch 105: loss = 0.3872390389442444\n",
      "epoch 106: loss = 0.38650012016296387\n",
      "epoch 107: loss = 0.38577401638031006\n",
      "epoch 108: loss = 0.38505637645721436\n",
      "epoch 109: loss = 0.38435041904449463\n",
      "epoch 110: loss = 0.3836570084095001\n",
      "epoch 111: loss = 0.3829691410064697\n",
      "epoch 112: loss = 0.3822939693927765\n",
      "epoch 113: loss = 0.38162335753440857\n",
      "epoch 114: loss = 0.3809654712677002\n",
      "epoch 115: loss = 0.38031652569770813\n",
      "epoch 116: loss = 0.3796730637550354\n",
      "epoch 117: loss = 0.37904077768325806\n",
      "epoch 118: loss = 0.37841591238975525\n",
      "epoch 119: loss = 0.37779802083969116\n",
      "epoch 120: loss = 0.3771907389163971\n",
      "epoch 121: loss = 0.37658631801605225\n",
      "epoch 122: loss = 0.37599584460258484\n",
      "epoch 123: loss = 0.37541040778160095\n",
      "epoch 124: loss = 0.3748317360877991\n",
      "epoch 125: loss = 0.3742605149745941\n",
      "epoch 126: loss = 0.37369731068611145\n",
      "epoch 127: loss = 0.37313807010650635\n",
      "epoch 128: loss = 0.37258610129356384\n",
      "epoch 129: loss = 0.3720420002937317\n",
      "epoch 130: loss = 0.3715047538280487\n",
      "epoch 131: loss = 0.37097376585006714\n",
      "epoch 132: loss = 0.37044757604599\n",
      "epoch 133: loss = 0.36992645263671875\n",
      "epoch 134: loss = 0.3694150745868683\n",
      "epoch 135: loss = 0.36890947818756104\n",
      "epoch 136: loss = 0.3684060871601105\n",
      "epoch 137: loss = 0.3679078221321106\n",
      "epoch 138: loss = 0.36741721630096436\n",
      "epoch 139: loss = 0.366934597492218\n",
      "epoch 140: loss = 0.36645451188087463\n",
      "epoch 141: loss = 0.36597782373428345\n",
      "epoch 142: loss = 0.36550915241241455\n",
      "epoch 143: loss = 0.3650436997413635\n",
      "epoch 144: loss = 0.36458447575569153\n",
      "epoch 145: loss = 0.3641292154788971\n",
      "epoch 146: loss = 0.3636796176433563\n",
      "epoch 147: loss = 0.36323362588882446\n",
      "epoch 148: loss = 0.3627932071685791\n",
      "epoch 149: loss = 0.3623568117618561\n",
      "epoch 150: loss = 0.36192378401756287\n",
      "epoch 151: loss = 0.3614944517612457\n",
      "epoch 152: loss = 0.3610713481903076\n",
      "epoch 153: loss = 0.36065369844436646\n",
      "epoch 154: loss = 0.3602372407913208\n",
      "epoch 155: loss = 0.3598281145095825\n",
      "epoch 156: loss = 0.35941991209983826\n",
      "epoch 157: loss = 0.3590170443058014\n",
      "epoch 158: loss = 0.35861679911613464\n",
      "epoch 159: loss = 0.35822269320487976\n",
      "epoch 160: loss = 0.3578302264213562\n",
      "epoch 161: loss = 0.35744354128837585\n",
      "epoch 162: loss = 0.35705792903900146\n",
      "epoch 163: loss = 0.35667556524276733\n",
      "epoch 164: loss = 0.35629943013191223\n",
      "epoch 165: loss = 0.3559277057647705\n",
      "epoch 166: loss = 0.35555678606033325\n",
      "epoch 167: loss = 0.3551916480064392\n",
      "epoch 168: loss = 0.35482868552207947\n",
      "epoch 169: loss = 0.35446444153785706\n",
      "epoch 170: loss = 0.354109525680542\n",
      "epoch 171: loss = 0.3537544310092926\n",
      "epoch 172: loss = 0.3534044921398163\n",
      "epoch 173: loss = 0.35305726528167725\n",
      "epoch 174: loss = 0.3527126610279083\n",
      "epoch 175: loss = 0.35236960649490356\n",
      "epoch 176: loss = 0.35203033685684204\n",
      "epoch 177: loss = 0.3516957461833954\n",
      "epoch 178: loss = 0.35136234760284424\n",
      "epoch 179: loss = 0.3510320782661438\n",
      "epoch 180: loss = 0.3507023751735687\n",
      "epoch 181: loss = 0.3503802716732025\n",
      "epoch 182: loss = 0.35005664825439453\n",
      "epoch 183: loss = 0.3497387170791626\n",
      "epoch 184: loss = 0.3494206368923187\n",
      "epoch 185: loss = 0.3491046726703644\n",
      "epoch 186: loss = 0.3487931787967682\n",
      "epoch 187: loss = 0.3484867513179779\n",
      "epoch 188: loss = 0.34818053245544434\n",
      "epoch 189: loss = 0.3478749096393585\n",
      "epoch 190: loss = 0.3475710153579712\n",
      "epoch 191: loss = 0.3472737669944763\n",
      "epoch 192: loss = 0.34697556495666504\n",
      "epoch 193: loss = 0.3466809391975403\n",
      "epoch 194: loss = 0.34638965129852295\n",
      "epoch 195: loss = 0.3460994064807892\n",
      "epoch 196: loss = 0.34581175446510315\n",
      "epoch 197: loss = 0.34552454948425293\n",
      "epoch 198: loss = 0.3452404737472534\n",
      "epoch 199: loss = 0.34495994448661804\n",
      "epoch 200: loss = 0.34468013048171997\n"
     ]
    }
   ],
   "source": [
    "train(model, torch.from_numpy(X_train).float(),torch.from_numpy(C_train),step_num=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率を調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044285714285715"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = model(torch.from_numpy(X_test).float())\n",
    "result = np.array([np.argmax(y) for y in Y_test.detach().numpy()])\n",
    "answer = C_test\n",
    "np.sum(np.equal(result, answer)) / C_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度はだいたい90%くらいになったことだろう。\n",
    "\n",
    "ネット上で同じデータ、同じロジスティック回帰を使ってテストした例では、92,3%くらいの精度になっているようだ。90%は少しだけ劣っているが、1つには学習方法もある。ここでは全データで一挙に重みを更新するバッチ学習というのを行っているが、これをいくつかのミニバッチに分割して勾配降下法を実行するSGD(確率的勾配降下法)がポピュラーな学習アルゴリズムで、局所最小値にはまる危険が少ない特長を持っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forwardネットワーク\n",
    "多クラスロジスティック回帰を少し拡張してみる。(記号を少し変えてある)\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_1&={\\boldsymbol{W}^{(1)}}^\\top\\boldsymbol{x} \\\\\n",
    "\\boldsymbol{z}_1&=\\sigma(\\boldsymbol{a}_1) \\\\\n",
    "\\boldsymbol{a}_2&={\\boldsymbol{W}^{(2)}}^\\top\\boldsymbol{z}_1^+ \\\\\n",
    "\\boldsymbol{y}&=\\text{softmax}(\\boldsymbol{a}_2)\n",
    "\\end{align*}\n",
    "$\\boldsymbol{x}$ → $\\boldsymbol{z_1}$ → $\\boldsymbol{y}$ の順でデータが変換されているのがわかるだろうか。\n",
    "\n",
    "$\\boldsymbol{W}^{(1)}$ は $(D+1)\\times H$行列、 $\\boldsymbol{W}^{(2)}$ は $(H+1)\\times M$行列である。(プラス1は切片のため)  $\\boldsymbol{z}^+$ は $\\boldsymbol{z}$ を1で拡張したベクトルを表す。$H$ は任意に決められるが、この数が重要であることが後でわかる。\n",
    "\n",
    "$\\sigma(\\cdot)$ はロジスティック関数であり、上式の $\\boldsymbol{z}_1=\\sigma(\\boldsymbol{a}_1)$ とは $\\boldsymbol{a}_1$ の各次元の値をロジスティック関数に通した値をつなげてベクトルにするという意味である。この関数は、PyTorchでは torch.nn.Sigmoid() として提供されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, C, learning_rate=0.5, step_num=100):\n",
    "    for i in range(step_num):\n",
    "        Y = model(X) # forward計算\n",
    "        loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "        W1, b1, W2, b2 = model.parameters()\n",
    "        if W1.grad is not None:\n",
    "            W1.grad.data.zero_()\n",
    "            b1.grad.data.zero_()\n",
    "        if W2.grad is not None:\n",
    "            W2.grad.data.zero_()\n",
    "            b2.grad.data.zero_()\n",
    "        loss.backward() # backward計算\n",
    "        # 勾配法\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        b1.data -= learning_rate * b1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        b2.data -= learning_rate * b2.grad.data\n",
    "\n",
    "        print(\"epoch {}: loss = {}\".format(i+1,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 500\n",
    "model = torch.nn.Sequential()\n",
    "model.add_module(\"linear1\", torch.nn.Linear(in_features=D,out_features=hidden_units))\n",
    "model.add_module(\"softmax1\", torch.nn.LogSoftmax(dim=1))\n",
    "model.add_module(\"linear2\", torch.nn.Linear(in_features=hidden_units,out_features=M))\n",
    "model.add_module(\"softmax2\", torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MNISTTrainDataSet(Dataset):\n",
    "    def __len__(self):\n",
    "        return X_train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X_train[idx].astype('float'), C_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50       # The size of input data took for one iteration\n",
    "\n",
    "mnist = MNISTTrainDataSet()\n",
    "train_loader = DataLoader(dataset=mnist,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/800], Loss: 1.9720\n",
      "Epoch [2/800], Loss: 1.7232\n",
      "Epoch [3/800], Loss: 1.4091\n",
      "Epoch [4/800], Loss: 1.2122\n",
      "Epoch [5/800], Loss: 1.0147\n",
      "Epoch [6/800], Loss: 0.9130\n",
      "Epoch [7/800], Loss: 0.8088\n",
      "Epoch [8/800], Loss: 0.7699\n",
      "Epoch [9/800], Loss: 0.7385\n",
      "Epoch [10/800], Loss: 0.6644\n",
      "Epoch [11/800], Loss: 0.6145\n",
      "Epoch [12/800], Loss: 0.5879\n",
      "Epoch [13/800], Loss: 0.5738\n",
      "Epoch [14/800], Loss: 0.5400\n",
      "Epoch [15/800], Loss: 0.5469\n",
      "Epoch [16/800], Loss: 0.5176\n",
      "Epoch [17/800], Loss: 0.4927\n",
      "Epoch [18/800], Loss: 0.4867\n",
      "Epoch [19/800], Loss: 0.4703\n",
      "Epoch [20/800], Loss: 0.4750\n",
      "Epoch [21/800], Loss: 0.4601\n",
      "Epoch [22/800], Loss: 0.4499\n",
      "Epoch [23/800], Loss: 0.4393\n",
      "Epoch [24/800], Loss: 0.4357\n",
      "Epoch [25/800], Loss: 0.4332\n",
      "Epoch [26/800], Loss: 0.4225\n",
      "Epoch [27/800], Loss: 0.4230\n",
      "Epoch [28/800], Loss: 0.4144\n",
      "Epoch [29/800], Loss: 0.4066\n",
      "Epoch [30/800], Loss: 0.4016\n",
      "Epoch [31/800], Loss: 0.4113\n",
      "Epoch [32/800], Loss: 0.4065\n",
      "Epoch [33/800], Loss: 0.3898\n",
      "Epoch [34/800], Loss: 0.3847\n",
      "Epoch [35/800], Loss: 0.3857\n",
      "Epoch [36/800], Loss: 0.3790\n",
      "Epoch [37/800], Loss: 0.3839\n",
      "Epoch [38/800], Loss: 0.3751\n",
      "Epoch [39/800], Loss: 0.3755\n",
      "Epoch [40/800], Loss: 0.3713\n",
      "Epoch [41/800], Loss: 0.3749\n",
      "Epoch [42/800], Loss: 0.3682\n",
      "Epoch [43/800], Loss: 0.3737\n",
      "Epoch [44/800], Loss: 0.3717\n",
      "Epoch [45/800], Loss: 0.3707\n",
      "Epoch [46/800], Loss: 0.3705\n",
      "Epoch [47/800], Loss: 0.3540\n",
      "Epoch [48/800], Loss: 0.3626\n",
      "Epoch [49/800], Loss: 0.3784\n",
      "Epoch [50/800], Loss: 0.3560\n",
      "Epoch [51/800], Loss: 0.3480\n",
      "Epoch [52/800], Loss: 0.3521\n",
      "Epoch [53/800], Loss: 0.3477\n",
      "Epoch [54/800], Loss: 0.3631\n",
      "Epoch [55/800], Loss: 0.3427\n",
      "Epoch [56/800], Loss: 0.3447\n",
      "Epoch [57/800], Loss: 0.3454\n",
      "Epoch [58/800], Loss: 0.3463\n",
      "Epoch [59/800], Loss: 0.3435\n",
      "Epoch [60/800], Loss: 0.3423\n",
      "Epoch [61/800], Loss: 0.3454\n",
      "Epoch [62/800], Loss: 0.3383\n",
      "Epoch [63/800], Loss: 0.3369\n",
      "Epoch [64/800], Loss: 0.3399\n",
      "Epoch [65/800], Loss: 0.3370\n",
      "Epoch [66/800], Loss: 0.3379\n",
      "Epoch [67/800], Loss: 0.3335\n",
      "Epoch [68/800], Loss: 0.3324\n",
      "Epoch [69/800], Loss: 0.3297\n",
      "Epoch [70/800], Loss: 0.3311\n",
      "Epoch [71/800], Loss: 0.3313\n",
      "Epoch [72/800], Loss: 0.3306\n",
      "Epoch [73/800], Loss: 0.3267\n",
      "Epoch [74/800], Loss: 0.3306\n",
      "Epoch [75/800], Loss: 0.3288\n",
      "Epoch [76/800], Loss: 0.3256\n",
      "Epoch [77/800], Loss: 0.3270\n",
      "Epoch [78/800], Loss: 0.3234\n",
      "Epoch [79/800], Loss: 0.3234\n",
      "Epoch [80/800], Loss: 0.3324\n",
      "Epoch [81/800], Loss: 0.3222\n",
      "Epoch [82/800], Loss: 0.3200\n",
      "Epoch [83/800], Loss: 0.3263\n",
      "Epoch [84/800], Loss: 0.3191\n",
      "Epoch [85/800], Loss: 0.3211\n",
      "Epoch [86/800], Loss: 0.3202\n",
      "Epoch [87/800], Loss: 0.3182\n",
      "Epoch [88/800], Loss: 0.3283\n",
      "Epoch [89/800], Loss: 0.3271\n",
      "Epoch [90/800], Loss: 0.3149\n",
      "Epoch [91/800], Loss: 0.3147\n",
      "Epoch [92/800], Loss: 0.3163\n",
      "Epoch [93/800], Loss: 0.3126\n",
      "Epoch [94/800], Loss: 0.3182\n",
      "Epoch [95/800], Loss: 0.3188\n",
      "Epoch [96/800], Loss: 0.3171\n",
      "Epoch [97/800], Loss: 0.3116\n",
      "Epoch [98/800], Loss: 0.3113\n",
      "Epoch [99/800], Loss: 0.3115\n",
      "Epoch [100/800], Loss: 0.3246\n",
      "Epoch [101/800], Loss: 0.3193\n",
      "Epoch [102/800], Loss: 0.3124\n",
      "Epoch [103/800], Loss: 0.3152\n",
      "Epoch [104/800], Loss: 0.3193\n",
      "Epoch [105/800], Loss: 0.3096\n",
      "Epoch [106/800], Loss: 0.3080\n",
      "Epoch [107/800], Loss: 0.3193\n",
      "Epoch [108/800], Loss: 0.3053\n",
      "Epoch [109/800], Loss: 0.3073\n",
      "Epoch [110/800], Loss: 0.3086\n",
      "Epoch [111/800], Loss: 0.3110\n",
      "Epoch [112/800], Loss: 0.3093\n",
      "Epoch [113/800], Loss: 0.3150\n",
      "Epoch [114/800], Loss: 0.3095\n",
      "Epoch [115/800], Loss: 0.3100\n",
      "Epoch [116/800], Loss: 0.3098\n",
      "Epoch [117/800], Loss: 0.3046\n",
      "Epoch [118/800], Loss: 0.3072\n",
      "Epoch [119/800], Loss: 0.3094\n",
      "Epoch [120/800], Loss: 0.3061\n",
      "Epoch [121/800], Loss: 0.3035\n",
      "Epoch [122/800], Loss: 0.3091\n",
      "Epoch [123/800], Loss: 0.3014\n",
      "Epoch [124/800], Loss: 0.3079\n",
      "Epoch [125/800], Loss: 0.3057\n",
      "Epoch [126/800], Loss: 0.3071\n",
      "Epoch [127/800], Loss: 0.3004\n",
      "Epoch [128/800], Loss: 0.3022\n",
      "Epoch [129/800], Loss: 0.3060\n",
      "Epoch [130/800], Loss: 0.3039\n",
      "Epoch [131/800], Loss: 0.3055\n",
      "Epoch [132/800], Loss: 0.3017\n",
      "Epoch [133/800], Loss: 0.3030\n",
      "Epoch [134/800], Loss: 0.3071\n",
      "Epoch [135/800], Loss: 0.2981\n",
      "Epoch [136/800], Loss: 0.2993\n",
      "Epoch [137/800], Loss: 0.3000\n",
      "Epoch [138/800], Loss: 0.3006\n",
      "Epoch [139/800], Loss: 0.2994\n",
      "Epoch [140/800], Loss: 0.3002\n",
      "Epoch [141/800], Loss: 0.3087\n",
      "Epoch [142/800], Loss: 0.3055\n",
      "Epoch [143/800], Loss: 0.3071\n",
      "Epoch [144/800], Loss: 0.2970\n",
      "Epoch [145/800], Loss: 0.2959\n",
      "Epoch [146/800], Loss: 0.2993\n",
      "Epoch [147/800], Loss: 0.2968\n",
      "Epoch [148/800], Loss: 0.2998\n",
      "Epoch [149/800], Loss: 0.3036\n",
      "Epoch [150/800], Loss: 0.2951\n",
      "Epoch [151/800], Loss: 0.2949\n",
      "Epoch [152/800], Loss: 0.2963\n",
      "Epoch [153/800], Loss: 0.2969\n",
      "Epoch [154/800], Loss: 0.2978\n",
      "Epoch [155/800], Loss: 0.2937\n",
      "Epoch [156/800], Loss: 0.3004\n",
      "Epoch [157/800], Loss: 0.2963\n",
      "Epoch [158/800], Loss: 0.2998\n",
      "Epoch [159/800], Loss: 0.3004\n",
      "Epoch [160/800], Loss: 0.2975\n",
      "Epoch [161/800], Loss: 0.2963\n",
      "Epoch [162/800], Loss: 0.2927\n",
      "Epoch [163/800], Loss: 0.3021\n",
      "Epoch [164/800], Loss: 0.2941\n",
      "Epoch [165/800], Loss: 0.3001\n",
      "Epoch [166/800], Loss: 0.2909\n",
      "Epoch [167/800], Loss: 0.2963\n",
      "Epoch [168/800], Loss: 0.2923\n",
      "Epoch [169/800], Loss: 0.3016\n",
      "Epoch [170/800], Loss: 0.2967\n",
      "Epoch [171/800], Loss: 0.2909\n",
      "Epoch [172/800], Loss: 0.2920\n",
      "Epoch [173/800], Loss: 0.2928\n",
      "Epoch [174/800], Loss: 0.2906\n",
      "Epoch [175/800], Loss: 0.2954\n",
      "Epoch [176/800], Loss: 0.2924\n",
      "Epoch [177/800], Loss: 0.2903\n",
      "Epoch [178/800], Loss: 0.2974\n",
      "Epoch [179/800], Loss: 0.2916\n",
      "Epoch [180/800], Loss: 0.2873\n",
      "Epoch [181/800], Loss: 0.2921\n",
      "Epoch [182/800], Loss: 0.2954\n",
      "Epoch [183/800], Loss: 0.2906\n",
      "Epoch [184/800], Loss: 0.2935\n",
      "Epoch [185/800], Loss: 0.2903\n",
      "Epoch [186/800], Loss: 0.2884\n",
      "Epoch [187/800], Loss: 0.2905\n",
      "Epoch [188/800], Loss: 0.2872\n",
      "Epoch [189/800], Loss: 0.2869\n",
      "Epoch [190/800], Loss: 0.2910\n",
      "Epoch [191/800], Loss: 0.2882\n",
      "Epoch [192/800], Loss: 0.2898\n",
      "Epoch [193/800], Loss: 0.2875\n",
      "Epoch [194/800], Loss: 0.2909\n",
      "Epoch [195/800], Loss: 0.2903\n",
      "Epoch [196/800], Loss: 0.2914\n",
      "Epoch [197/800], Loss: 0.2874\n",
      "Epoch [198/800], Loss: 0.2873\n",
      "Epoch [199/800], Loss: 0.2913\n",
      "Epoch [200/800], Loss: 0.2882\n",
      "Epoch [201/800], Loss: 0.2928\n",
      "Epoch [202/800], Loss: 0.2880\n",
      "Epoch [203/800], Loss: 0.2926\n",
      "Epoch [204/800], Loss: 0.2865\n",
      "Epoch [205/800], Loss: 0.2887\n",
      "Epoch [206/800], Loss: 0.2943\n",
      "Epoch [207/800], Loss: 0.2940\n",
      "Epoch [208/800], Loss: 0.2928\n",
      "Epoch [209/800], Loss: 0.2856\n",
      "Epoch [210/800], Loss: 0.2843\n",
      "Epoch [211/800], Loss: 0.2845\n",
      "Epoch [212/800], Loss: 0.2827\n",
      "Epoch [213/800], Loss: 0.2872\n",
      "Epoch [214/800], Loss: 0.2839\n",
      "Epoch [215/800], Loss: 0.2897\n",
      "Epoch [216/800], Loss: 0.2895\n",
      "Epoch [217/800], Loss: 0.2902\n",
      "Epoch [218/800], Loss: 0.2880\n",
      "Epoch [219/800], Loss: 0.2884\n",
      "Epoch [220/800], Loss: 0.2841\n",
      "Epoch [221/800], Loss: 0.2871\n",
      "Epoch [222/800], Loss: 0.2940\n",
      "Epoch [223/800], Loss: 0.2846\n",
      "Epoch [224/800], Loss: 0.2842\n",
      "Epoch [225/800], Loss: 0.2838\n",
      "Epoch [226/800], Loss: 0.2829\n",
      "Epoch [227/800], Loss: 0.2920\n",
      "Epoch [228/800], Loss: 0.2859\n",
      "Epoch [229/800], Loss: 0.2857\n",
      "Epoch [230/800], Loss: 0.2838\n",
      "Epoch [231/800], Loss: 0.2899\n",
      "Epoch [232/800], Loss: 0.2832\n",
      "Epoch [233/800], Loss: 0.2851\n",
      "Epoch [234/800], Loss: 0.2837\n",
      "Epoch [235/800], Loss: 0.2838\n",
      "Epoch [236/800], Loss: 0.2929\n",
      "Epoch [237/800], Loss: 0.2860\n",
      "Epoch [238/800], Loss: 0.2795\n",
      "Epoch [239/800], Loss: 0.2838\n",
      "Epoch [240/800], Loss: 0.2901\n",
      "Epoch [241/800], Loss: 0.2872\n",
      "Epoch [242/800], Loss: 0.2860\n",
      "Epoch [243/800], Loss: 0.2806\n",
      "Epoch [244/800], Loss: 0.2806\n",
      "Epoch [245/800], Loss: 0.2844\n",
      "Epoch [246/800], Loss: 0.2839\n",
      "Epoch [247/800], Loss: 0.2837\n",
      "Epoch [248/800], Loss: 0.2911\n",
      "Epoch [249/800], Loss: 0.2806\n",
      "Epoch [250/800], Loss: 0.2861\n",
      "Epoch [251/800], Loss: 0.2815\n",
      "Epoch [252/800], Loss: 0.2814\n",
      "Epoch [253/800], Loss: 0.2804\n",
      "Epoch [254/800], Loss: 0.2789\n",
      "Epoch [255/800], Loss: 0.2779\n",
      "Epoch [256/800], Loss: 0.2801\n",
      "Epoch [257/800], Loss: 0.2814\n",
      "Epoch [258/800], Loss: 0.2790\n",
      "Epoch [259/800], Loss: 0.2860\n",
      "Epoch [260/800], Loss: 0.2833\n",
      "Epoch [261/800], Loss: 0.2856\n",
      "Epoch [262/800], Loss: 0.2851\n",
      "Epoch [263/800], Loss: 0.2806\n",
      "Epoch [264/800], Loss: 0.2774\n",
      "Epoch [265/800], Loss: 0.2811\n",
      "Epoch [266/800], Loss: 0.2828\n",
      "Epoch [267/800], Loss: 0.2773\n",
      "Epoch [268/800], Loss: 0.2765\n",
      "Epoch [269/800], Loss: 0.2787\n",
      "Epoch [270/800], Loss: 0.2783\n",
      "Epoch [271/800], Loss: 0.2806\n",
      "Epoch [272/800], Loss: 0.2829\n",
      "Epoch [273/800], Loss: 0.2887\n",
      "Epoch [274/800], Loss: 0.2806\n",
      "Epoch [275/800], Loss: 0.2756\n",
      "Epoch [276/800], Loss: 0.2791\n",
      "Epoch [277/800], Loss: 0.2801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [278/800], Loss: 0.2796\n",
      "Epoch [279/800], Loss: 0.2773\n",
      "Epoch [280/800], Loss: 0.2793\n",
      "Epoch [281/800], Loss: 0.2759\n",
      "Epoch [282/800], Loss: 0.2788\n",
      "Epoch [283/800], Loss: 0.2812\n",
      "Epoch [284/800], Loss: 0.2806\n",
      "Epoch [285/800], Loss: 0.2759\n",
      "Epoch [286/800], Loss: 0.2821\n",
      "Epoch [287/800], Loss: 0.2793\n",
      "Epoch [288/800], Loss: 0.2779\n",
      "Epoch [289/800], Loss: 0.2788\n",
      "Epoch [290/800], Loss: 0.2771\n",
      "Epoch [291/800], Loss: 0.2809\n",
      "Epoch [292/800], Loss: 0.2786\n",
      "Epoch [293/800], Loss: 0.2811\n",
      "Epoch [294/800], Loss: 0.2791\n",
      "Epoch [295/800], Loss: 0.2776\n",
      "Epoch [296/800], Loss: 0.2798\n",
      "Epoch [297/800], Loss: 0.2755\n",
      "Epoch [298/800], Loss: 0.2739\n",
      "Epoch [299/800], Loss: 0.2758\n",
      "Epoch [300/800], Loss: 0.2745\n",
      "Epoch [301/800], Loss: 0.2758\n",
      "Epoch [302/800], Loss: 0.2825\n",
      "Epoch [303/800], Loss: 0.2754\n",
      "Epoch [304/800], Loss: 0.2765\n",
      "Epoch [305/800], Loss: 0.2755\n",
      "Epoch [306/800], Loss: 0.2775\n",
      "Epoch [307/800], Loss: 0.2767\n",
      "Epoch [308/800], Loss: 0.2737\n",
      "Epoch [309/800], Loss: 0.2790\n",
      "Epoch [310/800], Loss: 0.2756\n",
      "Epoch [311/800], Loss: 0.2730\n",
      "Epoch [312/800], Loss: 0.2731\n",
      "Epoch [313/800], Loss: 0.2750\n",
      "Epoch [314/800], Loss: 0.2748\n",
      "Epoch [315/800], Loss: 0.2742\n",
      "Epoch [316/800], Loss: 0.2726\n",
      "Epoch [317/800], Loss: 0.2959\n",
      "Epoch [318/800], Loss: 0.2735\n",
      "Epoch [319/800], Loss: 0.2810\n",
      "Epoch [320/800], Loss: 0.2776\n",
      "Epoch [321/800], Loss: 0.2728\n",
      "Epoch [322/800], Loss: 0.2765\n",
      "Epoch [323/800], Loss: 0.2745\n",
      "Epoch [324/800], Loss: 0.2769\n",
      "Epoch [325/800], Loss: 0.2736\n",
      "Epoch [326/800], Loss: 0.2745\n",
      "Epoch [327/800], Loss: 0.2722\n",
      "Epoch [328/800], Loss: 0.2750\n",
      "Epoch [329/800], Loss: 0.2788\n",
      "Epoch [330/800], Loss: 0.2753\n",
      "Epoch [331/800], Loss: 0.2771\n",
      "Epoch [332/800], Loss: 0.2768\n",
      "Epoch [333/800], Loss: 0.2722\n",
      "Epoch [334/800], Loss: 0.2732\n",
      "Epoch [335/800], Loss: 0.2829\n",
      "Epoch [336/800], Loss: 0.2766\n",
      "Epoch [337/800], Loss: 0.2748\n",
      "Epoch [338/800], Loss: 0.2734\n",
      "Epoch [339/800], Loss: 0.2755\n",
      "Epoch [340/800], Loss: 0.2717\n",
      "Epoch [341/800], Loss: 0.2708\n",
      "Epoch [342/800], Loss: 0.2723\n",
      "Epoch [343/800], Loss: 0.2710\n",
      "Epoch [344/800], Loss: 0.2723\n",
      "Epoch [345/800], Loss: 0.2716\n",
      "Epoch [346/800], Loss: 0.2723\n",
      "Epoch [347/800], Loss: 0.2728\n",
      "Epoch [348/800], Loss: 0.2829\n",
      "Epoch [349/800], Loss: 0.2731\n",
      "Epoch [350/800], Loss: 0.2714\n",
      "Epoch [351/800], Loss: 0.2756\n",
      "Epoch [352/800], Loss: 0.2723\n",
      "Epoch [353/800], Loss: 0.2697\n",
      "Epoch [354/800], Loss: 0.2792\n",
      "Epoch [355/800], Loss: 0.2703\n",
      "Epoch [356/800], Loss: 0.2692\n",
      "Epoch [357/800], Loss: 0.2720\n",
      "Epoch [358/800], Loss: 0.2717\n",
      "Epoch [359/800], Loss: 0.2726\n",
      "Epoch [360/800], Loss: 0.2696\n",
      "Epoch [361/800], Loss: 0.2773\n",
      "Epoch [362/800], Loss: 0.2765\n",
      "Epoch [363/800], Loss: 0.2751\n",
      "Epoch [364/800], Loss: 0.2737\n",
      "Epoch [365/800], Loss: 0.2754\n",
      "Epoch [366/800], Loss: 0.2699\n",
      "Epoch [367/800], Loss: 0.2714\n",
      "Epoch [368/800], Loss: 0.2737\n",
      "Epoch [369/800], Loss: 0.2822\n",
      "Epoch [370/800], Loss: 0.2736\n",
      "Epoch [371/800], Loss: 0.2697\n",
      "Epoch [372/800], Loss: 0.2760\n",
      "Epoch [373/800], Loss: 0.2734\n",
      "Epoch [374/800], Loss: 0.2699\n",
      "Epoch [375/800], Loss: 0.2678\n",
      "Epoch [376/800], Loss: 0.2723\n",
      "Epoch [377/800], Loss: 0.2703\n",
      "Epoch [378/800], Loss: 0.2691\n",
      "Epoch [379/800], Loss: 0.2744\n",
      "Epoch [380/800], Loss: 0.2718\n",
      "Epoch [381/800], Loss: 0.2716\n",
      "Epoch [382/800], Loss: 0.2723\n",
      "Epoch [383/800], Loss: 0.2724\n",
      "Epoch [384/800], Loss: 0.2716\n",
      "Epoch [385/800], Loss: 0.2743\n",
      "Epoch [386/800], Loss: 0.2771\n",
      "Epoch [387/800], Loss: 0.2765\n",
      "Epoch [388/800], Loss: 0.2712\n",
      "Epoch [389/800], Loss: 0.2700\n",
      "Epoch [390/800], Loss: 0.2713\n",
      "Epoch [391/800], Loss: 0.2738\n",
      "Epoch [392/800], Loss: 0.2697\n",
      "Epoch [393/800], Loss: 0.2686\n",
      "Epoch [394/800], Loss: 0.2692\n",
      "Epoch [395/800], Loss: 0.2741\n",
      "Epoch [396/800], Loss: 0.2678\n",
      "Epoch [397/800], Loss: 0.2663\n",
      "Epoch [398/800], Loss: 0.2722\n",
      "Epoch [399/800], Loss: 0.2685\n",
      "Epoch [400/800], Loss: 0.2664\n",
      "Epoch [401/800], Loss: 0.2728\n",
      "Epoch [402/800], Loss: 0.2698\n",
      "Epoch [403/800], Loss: 0.2708\n",
      "Epoch [404/800], Loss: 0.2696\n",
      "Epoch [405/800], Loss: 0.2676\n",
      "Epoch [406/800], Loss: 0.2839\n",
      "Epoch [407/800], Loss: 0.2727\n",
      "Epoch [408/800], Loss: 0.2706\n",
      "Epoch [409/800], Loss: 0.2688\n",
      "Epoch [410/800], Loss: 0.2683\n",
      "Epoch [411/800], Loss: 0.2709\n",
      "Epoch [412/800], Loss: 0.2660\n",
      "Epoch [413/800], Loss: 0.2725\n",
      "Epoch [414/800], Loss: 0.2660\n",
      "Epoch [415/800], Loss: 0.2685\n",
      "Epoch [416/800], Loss: 0.2724\n",
      "Epoch [417/800], Loss: 0.2687\n",
      "Epoch [418/800], Loss: 0.2782\n",
      "Epoch [419/800], Loss: 0.2686\n",
      "Epoch [420/800], Loss: 0.2720\n",
      "Epoch [421/800], Loss: 0.2666\n",
      "Epoch [422/800], Loss: 0.2703\n",
      "Epoch [423/800], Loss: 0.2720\n",
      "Epoch [424/800], Loss: 0.2683\n",
      "Epoch [425/800], Loss: 0.2687\n",
      "Epoch [426/800], Loss: 0.2703\n",
      "Epoch [427/800], Loss: 0.2696\n",
      "Epoch [428/800], Loss: 0.2685\n",
      "Epoch [429/800], Loss: 0.2789\n",
      "Epoch [430/800], Loss: 0.2676\n",
      "Epoch [431/800], Loss: 0.2690\n",
      "Epoch [432/800], Loss: 0.2664\n",
      "Epoch [433/800], Loss: 0.2656\n",
      "Epoch [434/800], Loss: 0.2689\n",
      "Epoch [435/800], Loss: 0.2679\n",
      "Epoch [436/800], Loss: 0.2663\n",
      "Epoch [437/800], Loss: 0.2735\n",
      "Epoch [438/800], Loss: 0.2670\n",
      "Epoch [439/800], Loss: 0.2678\n",
      "Epoch [440/800], Loss: 0.2689\n",
      "Epoch [441/800], Loss: 0.2777\n",
      "Epoch [442/800], Loss: 0.2699\n",
      "Epoch [443/800], Loss: 0.2691\n",
      "Epoch [444/800], Loss: 0.2667\n",
      "Epoch [445/800], Loss: 0.2637\n",
      "Epoch [446/800], Loss: 0.2667\n",
      "Epoch [447/800], Loss: 0.2634\n",
      "Epoch [448/800], Loss: 0.2654\n",
      "Epoch [449/800], Loss: 0.2706\n",
      "Epoch [450/800], Loss: 0.2643\n",
      "Epoch [451/800], Loss: 0.2675\n",
      "Epoch [452/800], Loss: 0.2727\n",
      "Epoch [453/800], Loss: 0.2808\n",
      "Epoch [454/800], Loss: 0.2685\n",
      "Epoch [455/800], Loss: 0.2707\n",
      "Epoch [456/800], Loss: 0.2639\n",
      "Epoch [457/800], Loss: 0.2665\n",
      "Epoch [458/800], Loss: 0.2729\n",
      "Epoch [459/800], Loss: 0.2702\n",
      "Epoch [460/800], Loss: 0.2687\n",
      "Epoch [461/800], Loss: 0.2659\n",
      "Epoch [462/800], Loss: 0.2635\n",
      "Epoch [463/800], Loss: 0.2672\n",
      "Epoch [464/800], Loss: 0.2702\n",
      "Epoch [465/800], Loss: 0.2671\n",
      "Epoch [466/800], Loss: 0.2723\n",
      "Epoch [467/800], Loss: 0.2705\n",
      "Epoch [468/800], Loss: 0.2711\n",
      "Epoch [469/800], Loss: 0.2704\n",
      "Epoch [470/800], Loss: 0.2694\n",
      "Epoch [471/800], Loss: 0.2695\n",
      "Epoch [472/800], Loss: 0.2756\n",
      "Epoch [473/800], Loss: 0.2727\n",
      "Epoch [474/800], Loss: 0.2647\n",
      "Epoch [475/800], Loss: 0.2713\n",
      "Epoch [476/800], Loss: 0.2659\n",
      "Epoch [477/800], Loss: 0.2640\n",
      "Epoch [478/800], Loss: 0.2682\n",
      "Epoch [479/800], Loss: 0.2663\n",
      "Epoch [480/800], Loss: 0.2724\n",
      "Epoch [481/800], Loss: 0.2685\n",
      "Epoch [482/800], Loss: 0.2663\n",
      "Epoch [483/800], Loss: 0.2646\n",
      "Epoch [484/800], Loss: 0.2753\n",
      "Epoch [485/800], Loss: 0.2658\n",
      "Epoch [486/800], Loss: 0.2625\n",
      "Epoch [487/800], Loss: 0.2661\n",
      "Epoch [488/800], Loss: 0.2658\n",
      "Epoch [489/800], Loss: 0.2695\n",
      "Epoch [490/800], Loss: 0.2655\n",
      "Epoch [491/800], Loss: 0.2689\n",
      "Epoch [492/800], Loss: 0.2792\n",
      "Epoch [493/800], Loss: 0.2635\n",
      "Epoch [494/800], Loss: 0.2647\n",
      "Epoch [495/800], Loss: 0.2647\n",
      "Epoch [496/800], Loss: 0.2674\n",
      "Epoch [497/800], Loss: 0.2649\n",
      "Epoch [498/800], Loss: 0.2684\n",
      "Epoch [499/800], Loss: 0.2776\n",
      "Epoch [500/800], Loss: 0.2613\n",
      "Epoch [501/800], Loss: 0.2623\n",
      "Epoch [502/800], Loss: 0.2660\n",
      "Epoch [503/800], Loss: 0.2640\n",
      "Epoch [504/800], Loss: 0.2648\n",
      "Epoch [505/800], Loss: 0.2623\n",
      "Epoch [506/800], Loss: 0.2659\n",
      "Epoch [507/800], Loss: 0.2618\n",
      "Epoch [508/800], Loss: 0.2663\n",
      "Epoch [509/800], Loss: 0.2613\n",
      "Epoch [510/800], Loss: 0.2606\n",
      "Epoch [511/800], Loss: 0.2628\n",
      "Epoch [512/800], Loss: 0.2688\n",
      "Epoch [513/800], Loss: 0.2651\n",
      "Epoch [514/800], Loss: 0.2619\n",
      "Epoch [515/800], Loss: 0.2690\n",
      "Epoch [516/800], Loss: 0.2657\n",
      "Epoch [517/800], Loss: 0.2634\n",
      "Epoch [518/800], Loss: 0.2620\n",
      "Epoch [519/800], Loss: 0.2728\n",
      "Epoch [520/800], Loss: 0.2627\n",
      "Epoch [521/800], Loss: 0.2701\n",
      "Epoch [522/800], Loss: 0.2630\n",
      "Epoch [523/800], Loss: 0.2647\n",
      "Epoch [524/800], Loss: 0.2635\n",
      "Epoch [525/800], Loss: 0.2649\n",
      "Epoch [526/800], Loss: 0.2638\n",
      "Epoch [527/800], Loss: 0.2637\n",
      "Epoch [528/800], Loss: 0.2619\n",
      "Epoch [529/800], Loss: 0.2641\n",
      "Epoch [530/800], Loss: 0.2715\n",
      "Epoch [531/800], Loss: 0.2659\n",
      "Epoch [532/800], Loss: 0.2621\n",
      "Epoch [533/800], Loss: 0.2607\n",
      "Epoch [534/800], Loss: 0.2611\n",
      "Epoch [535/800], Loss: 0.2688\n",
      "Epoch [536/800], Loss: 0.2625\n",
      "Epoch [537/800], Loss: 0.2717\n",
      "Epoch [538/800], Loss: 0.2591\n",
      "Epoch [539/800], Loss: 0.2602\n",
      "Epoch [540/800], Loss: 0.2618\n",
      "Epoch [541/800], Loss: 0.2670\n",
      "Epoch [542/800], Loss: 0.2660\n",
      "Epoch [543/800], Loss: 0.2581\n",
      "Epoch [544/800], Loss: 0.2705\n",
      "Epoch [545/800], Loss: 0.2632\n",
      "Epoch [546/800], Loss: 0.2593\n",
      "Epoch [547/800], Loss: 0.2689\n",
      "Epoch [548/800], Loss: 0.2655\n",
      "Epoch [549/800], Loss: 0.2624\n",
      "Epoch [550/800], Loss: 0.2670\n",
      "Epoch [551/800], Loss: 0.2595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [552/800], Loss: 0.2634\n",
      "Epoch [553/800], Loss: 0.2639\n",
      "Epoch [554/800], Loss: 0.2607\n",
      "Epoch [555/800], Loss: 0.2602\n",
      "Epoch [556/800], Loss: 0.2617\n",
      "Epoch [557/800], Loss: 0.2615\n",
      "Epoch [558/800], Loss: 0.2648\n",
      "Epoch [559/800], Loss: 0.2628\n",
      "Epoch [560/800], Loss: 0.2607\n",
      "Epoch [561/800], Loss: 0.2625\n",
      "Epoch [562/800], Loss: 0.2666\n",
      "Epoch [563/800], Loss: 0.2586\n",
      "Epoch [564/800], Loss: 0.2605\n",
      "Epoch [565/800], Loss: 0.2609\n",
      "Epoch [566/800], Loss: 0.2629\n",
      "Epoch [567/800], Loss: 0.2652\n",
      "Epoch [568/800], Loss: 0.2650\n",
      "Epoch [569/800], Loss: 0.2645\n",
      "Epoch [570/800], Loss: 0.2618\n",
      "Epoch [571/800], Loss: 0.2686\n",
      "Epoch [572/800], Loss: 0.2627\n",
      "Epoch [573/800], Loss: 0.2599\n",
      "Epoch [574/800], Loss: 0.2646\n",
      "Epoch [575/800], Loss: 0.2574\n",
      "Epoch [576/800], Loss: 0.2613\n",
      "Epoch [577/800], Loss: 0.2630\n",
      "Epoch [578/800], Loss: 0.2583\n",
      "Epoch [579/800], Loss: 0.2590\n",
      "Epoch [580/800], Loss: 0.2726\n",
      "Epoch [581/800], Loss: 0.2672\n",
      "Epoch [582/800], Loss: 0.2632\n",
      "Epoch [583/800], Loss: 0.2615\n",
      "Epoch [584/800], Loss: 0.2597\n",
      "Epoch [585/800], Loss: 0.2591\n",
      "Epoch [586/800], Loss: 0.2595\n",
      "Epoch [587/800], Loss: 0.2634\n",
      "Epoch [588/800], Loss: 0.2634\n",
      "Epoch [589/800], Loss: 0.2579\n",
      "Epoch [590/800], Loss: 0.2663\n",
      "Epoch [591/800], Loss: 0.2580\n",
      "Epoch [592/800], Loss: 0.2639\n",
      "Epoch [593/800], Loss: 0.2615\n",
      "Epoch [594/800], Loss: 0.2592\n",
      "Epoch [595/800], Loss: 0.2596\n",
      "Epoch [596/800], Loss: 0.2644\n",
      "Epoch [597/800], Loss: 0.2593\n",
      "Epoch [598/800], Loss: 0.2674\n",
      "Epoch [599/800], Loss: 0.2590\n",
      "Epoch [600/800], Loss: 0.2588\n",
      "Epoch [601/800], Loss: 0.2583\n",
      "Epoch [602/800], Loss: 0.2623\n",
      "Epoch [603/800], Loss: 0.2587\n",
      "Epoch [604/800], Loss: 0.2589\n",
      "Epoch [605/800], Loss: 0.2598\n",
      "Epoch [606/800], Loss: 0.2627\n",
      "Epoch [607/800], Loss: 0.2593\n",
      "Epoch [608/800], Loss: 0.2588\n",
      "Epoch [609/800], Loss: 0.2601\n",
      "Epoch [610/800], Loss: 0.2705\n",
      "Epoch [611/800], Loss: 0.2581\n",
      "Epoch [612/800], Loss: 0.2607\n",
      "Epoch [613/800], Loss: 0.2632\n",
      "Epoch [614/800], Loss: 0.2631\n",
      "Epoch [615/800], Loss: 0.2602\n",
      "Epoch [616/800], Loss: 0.2583\n",
      "Epoch [617/800], Loss: 0.2604\n",
      "Epoch [618/800], Loss: 0.2627\n",
      "Epoch [619/800], Loss: 0.2607\n",
      "Epoch [620/800], Loss: 0.2579\n",
      "Epoch [621/800], Loss: 0.2597\n",
      "Epoch [622/800], Loss: 0.2596\n",
      "Epoch [623/800], Loss: 0.2618\n",
      "Epoch [624/800], Loss: 0.2596\n",
      "Epoch [625/800], Loss: 0.2692\n",
      "Epoch [626/800], Loss: 0.2587\n",
      "Epoch [627/800], Loss: 0.2606\n",
      "Epoch [628/800], Loss: 0.2608\n",
      "Epoch [629/800], Loss: 0.2584\n",
      "Epoch [630/800], Loss: 0.2625\n",
      "Epoch [631/800], Loss: 0.2577\n",
      "Epoch [632/800], Loss: 0.2624\n",
      "Epoch [633/800], Loss: 0.2583\n",
      "Epoch [634/800], Loss: 0.2605\n",
      "Epoch [635/800], Loss: 0.2581\n",
      "Epoch [636/800], Loss: 0.2584\n",
      "Epoch [637/800], Loss: 0.2569\n",
      "Epoch [638/800], Loss: 0.2573\n",
      "Epoch [639/800], Loss: 0.2610\n",
      "Epoch [640/800], Loss: 0.2567\n",
      "Epoch [641/800], Loss: 0.2571\n",
      "Epoch [642/800], Loss: 0.2564\n",
      "Epoch [643/800], Loss: 0.2584\n",
      "Epoch [644/800], Loss: 0.2605\n",
      "Epoch [645/800], Loss: 0.2590\n",
      "Epoch [646/800], Loss: 0.2634\n",
      "Epoch [647/800], Loss: 0.2611\n",
      "Epoch [648/800], Loss: 0.2617\n",
      "Epoch [649/800], Loss: 0.2634\n",
      "Epoch [650/800], Loss: 0.2581\n",
      "Epoch [651/800], Loss: 0.2574\n",
      "Epoch [652/800], Loss: 0.2611\n",
      "Epoch [653/800], Loss: 0.2564\n",
      "Epoch [654/800], Loss: 0.2619\n",
      "Epoch [655/800], Loss: 0.2571\n",
      "Epoch [656/800], Loss: 0.2644\n",
      "Epoch [657/800], Loss: 0.2608\n",
      "Epoch [658/800], Loss: 0.2564\n",
      "Epoch [659/800], Loss: 0.2601\n",
      "Epoch [660/800], Loss: 0.2562\n",
      "Epoch [661/800], Loss: 0.2668\n",
      "Epoch [662/800], Loss: 0.2595\n",
      "Epoch [663/800], Loss: 0.2689\n",
      "Epoch [664/800], Loss: 0.2595\n",
      "Epoch [665/800], Loss: 0.2627\n",
      "Epoch [666/800], Loss: 0.2598\n",
      "Epoch [667/800], Loss: 0.2600\n",
      "Epoch [668/800], Loss: 0.2564\n",
      "Epoch [669/800], Loss: 0.2616\n",
      "Epoch [670/800], Loss: 0.2576\n",
      "Epoch [671/800], Loss: 0.2579\n",
      "Epoch [672/800], Loss: 0.2676\n",
      "Epoch [673/800], Loss: 0.2574\n",
      "Epoch [674/800], Loss: 0.2568\n",
      "Epoch [675/800], Loss: 0.2585\n",
      "Epoch [676/800], Loss: 0.2574\n",
      "Epoch [677/800], Loss: 0.2593\n",
      "Epoch [678/800], Loss: 0.2579\n",
      "Epoch [679/800], Loss: 0.2593\n",
      "Epoch [680/800], Loss: 0.2641\n",
      "Epoch [681/800], Loss: 0.2592\n",
      "Epoch [682/800], Loss: 0.2584\n",
      "Epoch [683/800], Loss: 0.2596\n",
      "Epoch [684/800], Loss: 0.2595\n",
      "Epoch [685/800], Loss: 0.2566\n",
      "Epoch [686/800], Loss: 0.2592\n",
      "Epoch [687/800], Loss: 0.2591\n",
      "Epoch [688/800], Loss: 0.2582\n",
      "Epoch [689/800], Loss: 0.2583\n",
      "Epoch [690/800], Loss: 0.2622\n",
      "Epoch [691/800], Loss: 0.2558\n",
      "Epoch [692/800], Loss: 0.2609\n",
      "Epoch [693/800], Loss: 0.2554\n",
      "Epoch [694/800], Loss: 0.2650\n",
      "Epoch [695/800], Loss: 0.2566\n",
      "Epoch [696/800], Loss: 0.2596\n",
      "Epoch [697/800], Loss: 0.2610\n",
      "Epoch [698/800], Loss: 0.2564\n",
      "Epoch [699/800], Loss: 0.2575\n",
      "Epoch [700/800], Loss: 0.2608\n",
      "Epoch [701/800], Loss: 0.2569\n",
      "Epoch [702/800], Loss: 0.2567\n",
      "Epoch [703/800], Loss: 0.2565\n",
      "Epoch [704/800], Loss: 0.2590\n",
      "Epoch [705/800], Loss: 0.2577\n",
      "Epoch [706/800], Loss: 0.2564\n",
      "Epoch [707/800], Loss: 0.2599\n",
      "Epoch [708/800], Loss: 0.2540\n",
      "Epoch [709/800], Loss: 0.2582\n",
      "Epoch [710/800], Loss: 0.2542\n",
      "Epoch [711/800], Loss: 0.2596\n",
      "Epoch [712/800], Loss: 0.2555\n",
      "Epoch [713/800], Loss: 0.2587\n",
      "Epoch [714/800], Loss: 0.2661\n",
      "Epoch [715/800], Loss: 0.2596\n",
      "Epoch [716/800], Loss: 0.2612\n",
      "Epoch [717/800], Loss: 0.2550\n",
      "Epoch [718/800], Loss: 0.2591\n",
      "Epoch [719/800], Loss: 0.2553\n",
      "Epoch [720/800], Loss: 0.2602\n",
      "Epoch [721/800], Loss: 0.2610\n",
      "Epoch [722/800], Loss: 0.2568\n",
      "Epoch [723/800], Loss: 0.2557\n",
      "Epoch [724/800], Loss: 0.2621\n",
      "Epoch [725/800], Loss: 0.2555\n",
      "Epoch [726/800], Loss: 0.2535\n",
      "Epoch [727/800], Loss: 0.2606\n",
      "Epoch [728/800], Loss: 0.2563\n",
      "Epoch [729/800], Loss: 0.2561\n",
      "Epoch [730/800], Loss: 0.2529\n",
      "Epoch [731/800], Loss: 0.2546\n",
      "Epoch [732/800], Loss: 0.2540\n",
      "Epoch [733/800], Loss: 0.2655\n",
      "Epoch [734/800], Loss: 0.2593\n",
      "Epoch [735/800], Loss: 0.2581\n",
      "Epoch [736/800], Loss: 0.2568\n",
      "Epoch [737/800], Loss: 0.2603\n",
      "Epoch [738/800], Loss: 0.2541\n",
      "Epoch [739/800], Loss: 0.2575\n",
      "Epoch [740/800], Loss: 0.2575\n",
      "Epoch [741/800], Loss: 0.2547\n",
      "Epoch [742/800], Loss: 0.2560\n",
      "Epoch [743/800], Loss: 0.2534\n",
      "Epoch [744/800], Loss: 0.2526\n",
      "Epoch [745/800], Loss: 0.2555\n",
      "Epoch [746/800], Loss: 0.2538\n",
      "Epoch [747/800], Loss: 0.2537\n",
      "Epoch [748/800], Loss: 0.2564\n",
      "Epoch [749/800], Loss: 0.2607\n",
      "Epoch [750/800], Loss: 0.2620\n",
      "Epoch [751/800], Loss: 0.2581\n",
      "Epoch [752/800], Loss: 0.2566\n",
      "Epoch [753/800], Loss: 0.2567\n",
      "Epoch [754/800], Loss: 0.2557\n",
      "Epoch [755/800], Loss: 0.2578\n",
      "Epoch [756/800], Loss: 0.2565\n",
      "Epoch [757/800], Loss: 0.2529\n",
      "Epoch [758/800], Loss: 0.2586\n",
      "Epoch [759/800], Loss: 0.2571\n",
      "Epoch [760/800], Loss: 0.2530\n",
      "Epoch [761/800], Loss: 0.2563\n",
      "Epoch [762/800], Loss: 0.2675\n",
      "Epoch [763/800], Loss: 0.2625\n",
      "Epoch [764/800], Loss: 0.2539\n",
      "Epoch [765/800], Loss: 0.2621\n",
      "Epoch [766/800], Loss: 0.2562\n",
      "Epoch [767/800], Loss: 0.2536\n",
      "Epoch [768/800], Loss: 0.2587\n",
      "Epoch [769/800], Loss: 0.2525\n",
      "Epoch [770/800], Loss: 0.2530\n",
      "Epoch [771/800], Loss: 0.2537\n",
      "Epoch [772/800], Loss: 0.2567\n",
      "Epoch [773/800], Loss: 0.2569\n",
      "Epoch [774/800], Loss: 0.2630\n",
      "Epoch [775/800], Loss: 0.2608\n",
      "Epoch [776/800], Loss: 0.2543\n",
      "Epoch [777/800], Loss: 0.2541\n",
      "Epoch [778/800], Loss: 0.2522\n",
      "Epoch [779/800], Loss: 0.2564\n",
      "Epoch [780/800], Loss: 0.2549\n",
      "Epoch [781/800], Loss: 0.2584\n",
      "Epoch [782/800], Loss: 0.2547\n",
      "Epoch [783/800], Loss: 0.2531\n",
      "Epoch [784/800], Loss: 0.2536\n",
      "Epoch [785/800], Loss: 0.2630\n",
      "Epoch [786/800], Loss: 0.2563\n",
      "Epoch [787/800], Loss: 0.2569\n",
      "Epoch [788/800], Loss: 0.2553\n",
      "Epoch [789/800], Loss: 0.2524\n",
      "Epoch [790/800], Loss: 0.2563\n",
      "Epoch [791/800], Loss: 0.2562\n",
      "Epoch [792/800], Loss: 0.2544\n",
      "Epoch [793/800], Loss: 0.2541\n",
      "Epoch [794/800], Loss: 0.2609\n",
      "Epoch [795/800], Loss: 0.2516\n",
      "Epoch [796/800], Loss: 0.2541\n",
      "Epoch [797/800], Loss: 0.2539\n",
      "Epoch [798/800], Loss: 0.2541\n",
      "Epoch [799/800], Loss: 0.2550\n",
      "Epoch [800/800], Loss: 0.2578\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 800        # The number of times entire dataset is trained\n",
    "learning_rate = 0.0005  # The speed of convergence\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, C) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        X = X.float()\n",
    "        Y = model(X) # forward計算\n",
    "        loss = crossEntropyLoss(Y, C) # 損失の計算\n",
    "        W1, b1, W2, b2 = model.parameters()\n",
    "        if W1.grad is not None:\n",
    "            W1.grad.data.zero_()\n",
    "            b1.grad.data.zero_()\n",
    "        if W2.grad is not None:\n",
    "            W2.grad.data.zero_()\n",
    "            b2.grad.data.zero_()\n",
    "        loss.backward() # backward計算\n",
    "        # 勾配法\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        b1.data -= learning_rate * b1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        b2.data -= learning_rate * b2.grad.data\n",
    "    Y = model(torch.from_numpy(X_train).float())\n",
    "    C = torch.from_numpy(C_train)\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, crossEntropyLoss(Y, C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, torch.from_numpy(X_train).float(),torch.from_numpy(C_train),learning_rate=0.001,step_num=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192857142857143"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = model(torch.from_numpy(X_test).float())\n",
    "result = np.array([np.argmax(y) for y in Y_test.detach().numpy()])\n",
    "answer = C_test\n",
    "np.sum(np.equal(result, answer)) / C_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題\n",
    "1. Feed-forwardネットワークにより手書き数字画像のモデル化を行い、多クラスロジスティック回帰との性能を比較せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
