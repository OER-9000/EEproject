{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カテゴリカル分布\n",
    "goodかbadの二択問題で、goodの確率が $\\mu$ であるとき、goodである確率とbadである確率をまとめて書くと\n",
    "<span id=\"eq1\">\n",
    "\\begin{equation}\n",
    "P(t)=\\mu^t(1-\\mu)^{1-t}\n",
    "\\end{equation}\n",
    "</span>\n",
    "と書けた。\n",
    "\n",
    "次に、三択以上の確率分布を考えよう。サイコロの目のように、出目が6通りあるとする。このときの出目を、以下のようなone-hot表現で表すことにする。\n",
    "$$\\boldsymbol{x}=(0, 0, 1, 0, 0, 0)^\\top$$\n",
    "\n",
    "このとき、one-hot表現の出目ベクトル $\\boldsymbol{x}$ の確率は\n",
    "<span id=\"eq2\">\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{x}|\\boldsymbol{\\mu})=\\prod_{m=1}^6 \\mu_m^{x_m}\n",
    "\\end{equation}\n",
    "</span>\n",
    "となる。 $\\boldsymbol{\\mu}=(\\mu_1,\\mu_2,\\cdots)$ はパラメータベクトルである。\n",
    "\n",
    "$K$ 回サイコロを振ったデータをまとめて $\\boldsymbol{\\mathsf{X}}$ と書くことにすると、このモデルから $\\boldsymbol{\\mathsf{X}}$ が発生する同時確率は\n",
    "<span id=\"eq3\">\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{\\mathsf{X}}|\\boldsymbol{\\mu})=\\prod_{k=1}^K\\prod_{m=1}^6 \\mu_m^{x_{km}}\n",
    "\\end{equation}\n",
    "</span>\n",
    "\n",
    "この同時確率を $\\boldsymbol{\\mu}$ の関数とみなし、これを尤度関数という。よって対数尤度関数は\n",
    "<span id=\"eq4\">\n",
    "\\begin{equation}\n",
    "\\ln L(\\boldsymbol{\\mu})=\\ln P(\\boldsymbol{\\mathsf{X}}|\\boldsymbol{\\mu})=\\sum_{k=1}^K\\sum_{m=1}^6 x_{km}\\ln\\mu_m\n",
    "\\end{equation}\n",
    "</span>\n",
    "この関数を最大にするような $\\boldsymbol{\\mu}$ を求めればよい。\n",
    "\n",
    "ところで、$m$ の目が $c_m$ 回出たとすると、 $c_m=\\sum_{k=1}^K x_{km}$ だから\n",
    "<span id=\"eq5\">\n",
    "\\begin{equation}\n",
    "\\ln L(\\boldsymbol{\\mu})=\\sum_{m=1}^6 c_m\\ln\\mu_m\n",
    "\\end{equation}\n",
    "</span>\n",
    "と書いてもよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配法によるパラメータ推定\n",
    "尤度を最大にする $\\boldsymbol{\\mu}$ は解析的に求まり、\n",
    "<span id=\"eq-ml\">\n",
    "\\begin{equation}\n",
    "\\hat{\\mu}_m=\\frac{c_m}{K}\n",
    "\\end{equation}\n",
    "</span>\n",
    "である([課題](#kadai2))。わざわざパラメータ推定を勾配法で行う必要は全くないのだが、今回は練習のために敢えて勾配法で求めてみる。\n",
    "\n",
    "6通りの出目を持つサイコロを100回振った結果から、パラメータ(=出目の確率)を推定しよう。まず、$\\boldsymbol{\\mu}$ の真値を決めておく。全部等確率だとおもしろくないので、2の目の確率を0.5, そのほかを0.1とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 59, 10,  5,  8,  8])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu_true = [0.1, 0.5, 0.1, 0.1, 0.1, 0.1]\n",
    "c = np.random.multinomial(100, mu_true)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、[式(2)](#eq3)の確率分布にしたがって100回サイコロを振る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1, 2, 2, 2, 2, 2, 2, 6, 3, 3, 5, 2, 4, 2, 3, 2, 5, 5, 2, 2,\n",
       "       5, 2, 2, 5, 4, 6, 6, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 3, 1, 3, 4, 1,\n",
       "       4, 5, 3, 1, 2, 6, 2, 6, 3, 1, 4, 4, 2, 2, 2, 2, 2, 5, 2, 6, 6, 4,\n",
       "       2, 2, 2, 2, 2, 1, 2, 2, 6, 2, 6, 3, 1, 1, 4, 3, 3, 2, 2, 2, 2, 3,\n",
       "       2, 3, 3, 2, 6, 1, 5, 3, 2, 4, 5, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk = np.random.choice(6,100,p=mu_true)\n",
    "xk+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出目の回数は bincount 関数で数えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 45, 14, 10,  9, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.bincount(xk)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xkをone-hot表現に変換しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.identity(6)[xk]\n",
    "X[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[式(4)](#eq4)にしたがい、Xがデータとして与えられたときの対数尤度関数は次のように定義できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_likelihood(mu):\n",
    "    return np.sum(X * np.log(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hotベクトルでなく、出目の回数cがデータとして与えられた場合の対数尤度関数は、[式(5)](#eq5)より次のように定義できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_likelihood(mu):\n",
    "    return np.sum(c * np.log(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらが同じ結果を与えることを確認しておこう。適当な $\\mu$ に対して"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-202.349172006833\n",
      "-202.349172006833\n"
     ]
    }
   ],
   "source": [
    "mu = np.array([0.2,0.1,0.2,0.1,0.3,0.1])\n",
    "print(categorical_likelihood(mu))\n",
    "print(multinomial_likelihood(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ結果になった。以下ではデータとしてcを与えることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「ゼロから作るDeep Learning」 p.104\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1.0e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「ゼロから作るDeep Learning」 p.107\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.001, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は負の対数尤度で定義し、categorical_entropy_lossという名前を付けておく。\n",
    "\n",
    "ここで1つコツがある。 $\\mu_1,\\mu_2,\\cdots$ の総和は1でなければならないが、モデルパラメータにそのような制約を入れるのは少し厄介である。そこで、パラメータは和が1にならなくてもよいこととし、それらを $\\mu^\\text{raw}_{1}, \\mu^\\text{raw}_{2},\\cdots$ と書く(つまり $\\mu^\\text{raw}_{1}$ 等は確率ではない)。そして、確率は\n",
    "$$\\mu_m = \\frac{\\mu^\\text{raw}_{m}}{\\sum_{m=1}^6\\mu^\\text{raw}_{m}}$$\n",
    "のようにして計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_entropy_loss(mu_raw):\n",
    "    mu = mu_raw / np.sum(mu_raw)\n",
    "    return -multinomial_likelihood(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、パラメータを推定してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12000001, 0.44999995, 0.14000002, 0.10000001, 0.09      ,\n",
       "       0.10000001])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_raw = gradient_descent(categorical_entropy_loss, np.random.rand(6),lr=0.01)\n",
    "mu = mu_raw / np.sum(mu_raw)\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 多クラスロジスティック回帰\n",
    "ロジスティック回帰を多クラス問題に拡張しよう。多クラスロジスティック回帰モデルは、線形予測子 $\\boldsymbol{z}=(z_1 z_2 \\cdots z_M)^\\top$ および softmax 関数 $\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}$ により以下のように書くことができる ($m$ はクラス番号)。\n",
    "<span id=\"multiclasslogistic\">\n",
    "\\begin{align*}\n",
    "z_m&=\\boldsymbol{w_m}^\\top\\boldsymbol{x} \\\\\n",
    "y_m&=\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}\n",
    "\\end{align*}\n",
    "</span>\n",
    "\n",
    "ロジスティック回帰で仮定した確率の[式(1)](#eq1)を[式(2)](#eq2)に置き換える。説明変数 $\\boldsymbol{x}$ によって多クラスロジスティック回帰モデルで予測されるクラス ${\\cal C}$ の確率は\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{t}|\\boldsymbol{w})=\\prod_m y_m^{t_m}\n",
    "\\end{equation}\n",
    "ただし $\\boldsymbol{t}=(t_1 t_2 \\cdots)^\\top$ はクラス ${\\cal C}$ の one-hot 表現である。\n",
    "\n",
    "データセット $\\boldsymbol{\\mathsf{X}}=(\\boldsymbol{x}_1 \\boldsymbol{x}_2 \\cdots)$,\n",
    "$\\boldsymbol{\\mathsf{T}}=(\\boldsymbol{t}_1 \\boldsymbol{t}_2 \\cdots)$ に対し、 $\\boldsymbol{x}_1, \\boldsymbol{x}_2 \\cdots$ のそれぞれが $\\boldsymbol{t}_1, \\boldsymbol{t}_2 \\cdots$ になる同時確率は、それぞれの確率の積になるから\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{\\mathsf{T}}|\\boldsymbol{w})=\\prod_k \\prod_m y_{km}^{t_{km}}\n",
    "\\end{equation}\n",
    "\n",
    "よって、交差エントロピー(=負の対数尤度)は\n",
    "<span id=\"eq-ent\">\n",
    "\\begin{equation}\n",
    "E=-\\ln P(\\boldsymbol{\\mathsf{T}}|\\boldsymbol{w})=-\\sum_k \\sum_m t_{km} \\ln y_{km}\n",
    "\\end{equation}\n",
    "</span>\n",
    "となる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris (アヤメの判別)\n",
    "ロジスティック回帰を用いてアヤメの花の寸法から3種類のアヤメを判別する問題を解いてみよう。irisデータセットについては各自調べておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "T = np.identity(3)[iris.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "説明変数は4つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数tは3種類のアヤメの種類のone-hot表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]]), array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0:5,:] , T[145:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0::2] # 偶数番目\n",
    "X_test = X[1::2] # 奇数番目\n",
    "T_train = T[0::2]\n",
    "T_test = T[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax関数を定義する。戻り値はクラス数を $M$ とすると $M$ 次元のベクトルになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[式(9)](#eq-ent)に従って損失関数を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_loss(W):\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ $\\boldsymbol{W}=(\\boldsymbol{w}_1 \\boldsymbol{w}_2 \\boldsymbol{w}_3)$ は今度は行列になるので、多次元配列に対応したバージョンの numerical_gradient を使う。([ここ](https://github.com/oreilly-japan/deep-learning-from-scratch)から入手できます)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a = np.column_stack((np.ones(X_train.shape[0]),X_train))\n",
    "X_test_a = np.column_stack((np.ones(X_test.shape[0]),X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=gradient_descent(categorical_cross_entropy_loss, np.random.rand(5,3), lr=0.001,step_num=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに対してロジスティック回帰モデルによる予測を行う。各データが各クラスの花である確率は次のように計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9.74597707e-01, 2.54022892e-02, 4.22062668e-09]),\n",
       " array([9.74765757e-01, 2.52342320e-02, 1.06202665e-08]),\n",
       " array([9.93588421e-01, 6.41157920e-03, 2.90446617e-10]),\n",
       " array([9.88059504e-01, 1.19404949e-02, 9.16534435e-10]),\n",
       " array([9.76139723e-01, 2.38602737e-02, 2.84919748e-09]),\n",
       " array([9.84665827e-01, 1.53341701e-02, 2.81968790e-09]),\n",
       " array([9.89087634e-01, 1.09123640e-02, 2.05323312e-09]),\n",
       " array([9.98932441e-01, 1.06755902e-03, 3.82799086e-12]),\n",
       " array([9.92340290e-01, 7.65970996e-03, 4.10561595e-10]),\n",
       " array([9.95403502e-01, 4.59649776e-03, 1.83612685e-10]),\n",
       " array([9.93660028e-01, 6.33997175e-03, 4.94902921e-10]),\n",
       " array([9.68932838e-01, 3.10671445e-02, 1.76149657e-08]),\n",
       " array([9.57959063e-01, 4.20409261e-02, 1.05491990e-08]),\n",
       " array([9.90695401e-01, 9.30459851e-03, 3.38984426e-10]),\n",
       " array([9.74660588e-01, 2.53394029e-02, 9.28441528e-09]),\n",
       " array([9.86256140e-01, 1.37438594e-02, 9.07529165e-10]),\n",
       " array([9.98797471e-01, 1.20252892e-03, 3.19223693e-12]),\n",
       " array([9.90787420e-01, 9.21257927e-03, 3.89464323e-10]),\n",
       " array([9.76139723e-01, 2.38602737e-02, 2.84919748e-09]),\n",
       " array([9.88026788e-01, 1.19732112e-02, 7.03687777e-10]),\n",
       " array([8.85773418e-01, 1.14226309e-01, 2.72857894e-07]),\n",
       " array([9.84503096e-01, 1.54968953e-02, 8.47322486e-09]),\n",
       " array([9.72992227e-01, 2.70077640e-02, 9.27207027e-09]),\n",
       " array([9.84794151e-01, 1.52058457e-02, 3.68914421e-09]),\n",
       " array([9.88077161e-01, 1.19228379e-02, 8.04889288e-10]),\n",
       " array([0.00820621, 0.96398321, 0.02781058]),\n",
       " array([0.00308558, 0.86231654, 0.13459788]),\n",
       " array([0.00299968, 0.84930383, 0.14769649]),\n",
       " array([0.03349709, 0.95250479, 0.01399812]),\n",
       " array([0.01039305, 0.84705854, 0.14254841]),\n",
       " array([0.01046872, 0.9331864 , 0.05634488]),\n",
       " array([0.00218501, 0.86691171, 0.13090328]),\n",
       " array([0.00888503, 0.98403093, 0.00708404]),\n",
       " array([0.00916361, 0.98037019, 0.0104662 ]),\n",
       " array([0.00856135, 0.96982668, 0.02161197]),\n",
       " array([0.0124657 , 0.97684854, 0.01068576]),\n",
       " array([0.00206325, 0.92873104, 0.06920572]),\n",
       " array([0.00687001, 0.98172878, 0.01140122]),\n",
       " array([0.00095203, 0.78652861, 0.21251936]),\n",
       " array([0.03308507, 0.96474972, 0.00216521]),\n",
       " array([0.01198294, 0.97676453, 0.01125253]),\n",
       " array([1.19470037e-04, 2.54431384e-01, 7.45449146e-01]),\n",
       " array([0.01226081, 0.90864748, 0.07909171]),\n",
       " array([0.00115694, 0.93198926, 0.0668538 ]),\n",
       " array([0.0054049, 0.9022839, 0.0923112]),\n",
       " array([0.00387232, 0.91636261, 0.07976507]),\n",
       " array([0.02603741, 0.9606113 , 0.01335129]),\n",
       " array([0.01321315, 0.96177236, 0.0250145 ]),\n",
       " array([0.00737335, 0.9743248 , 0.01830185]),\n",
       " array([0.00944172, 0.94817494, 0.04238333]),\n",
       " array([1.86479478e-05, 4.80995248e-02, 9.51881827e-01]),\n",
       " array([1.46305621e-05, 7.81921911e-02, 9.21793178e-01]),\n",
       " array([3.98273667e-07, 2.77211425e-02, 9.72278459e-01]),\n",
       " array([3.02799057e-06, 1.02057965e-01, 8.97939007e-01]),\n",
       " array([5.63939513e-06, 2.94563234e-02, 9.70538037e-01]),\n",
       " array([2.56158055e-05, 1.12799999e-01, 8.87174385e-01]),\n",
       " array([6.20389210e-06, 2.20162042e-02, 9.77977592e-01]),\n",
       " array([3.49056160e-05, 5.51313558e-02, 9.44833739e-01]),\n",
       " array([6.76917903e-06, 8.32421432e-02, 9.16751088e-01]),\n",
       " array([3.73997411e-05, 2.08710292e-01, 7.91252308e-01]),\n",
       " array([3.46004657e-05, 4.36273153e-02, 9.56338084e-01]),\n",
       " array([2.56828361e-04, 3.74590334e-01, 6.25152837e-01]),\n",
       " array([4.37973659e-05, 3.12365857e-01, 6.87590345e-01]),\n",
       " array([5.94449684e-04, 3.98072250e-01, 6.01333301e-01]),\n",
       " array([9.54415194e-05, 5.95090540e-01, 4.04814018e-01]),\n",
       " array([1.03895846e-04, 5.17760758e-01, 4.82135346e-01]),\n",
       " array([3.84447752e-04, 5.97496664e-01, 4.02118888e-01]),\n",
       " array([3.45039071e-06, 7.51764109e-02, 9.24820139e-01]),\n",
       " array([7.76455812e-05, 1.91827106e-01, 8.08095249e-01]),\n",
       " array([1.02940138e-04, 2.42357987e-01, 7.57539073e-01]),\n",
       " array([2.11852859e-04, 2.61434656e-01, 7.38353491e-01]),\n",
       " array([2.86303317e-06, 2.16454192e-02, 9.78351718e-01]),\n",
       " array([5.19393997e-05, 1.06867356e-01, 8.93080704e-01]),\n",
       " array([1.29379356e-04, 2.17372338e-01, 7.82498282e-01]),\n",
       " array([1.47009536e-04, 1.64384956e-01, 8.35468035e-01])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[softmax(x) for x in X_test_a.dot(W)]\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{y}=(y_1 y_2 y_3)$ の中で確率が最大のクラスが認識結果となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.array([np.argmax(y) for y in Y])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解精度を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.equal(result, [np.argmax(t) for t in T_test])) / T_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "logreg.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.865244646419518"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(T_train * logreg.predict_log_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.83270063197948"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(T_train * np.log(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題\n",
    "1. (基本) [softmax関数の式](#multiclasslogistic)で、クラスの数を2とするとロジスティック関数と同じになることを示せ。\n",
    "1. <span id=\"kadai2\">(発展)</span> [カテゴリカル分布の最尤推定式](#eq-ml)を証明せよ。(ヒント: この問題は制約付き最大化問題になるのでLagrangeの未定乗数法を使うと解ける。)\n",
    "1. (基本) 上で未定義のまま残しておいた categorical_cross_entropy_loss 関数を完成させ、アヤメの判別を実行せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
