{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カテゴリカル分布\n",
    "goodかbadの二択問題で、goodの確率が $\\mu$ であるとき、goodである確率とbadである確率をまとめて書くと\n",
    "<span id=\"eq1\">\n",
    "\\begin{equation}\n",
    "P(t)=\\mu^t(1-\\mu)^{1-t}\n",
    "\\end{equation}\n",
    "</span>\n",
    "と書けた。\n",
    "\n",
    "次に、三択以上の確率分布を考えよう。サイコロの目のように、出目が6通りあるとする。このときの出目を、以下のようなone-hot表現で表すことにする。\n",
    "$$\\boldsymbol{x}=(0, 0, 1, 0, 0, 0)^\\top$$\n",
    "\n",
    "このとき、one-hot表現の出目ベクトル $\\boldsymbol{x}$ の確率は\n",
    "<span id=\"eq2\">\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{x}|\\boldsymbol{\\mu})=\\prod_{m=1}^6 \\mu_m^{x_m}\n",
    "\\end{equation}\n",
    "</span>\n",
    "となる。 $\\boldsymbol{\\mu}=(\\mu_1,\\mu_2,\\cdots)$ はパラメータベクトルである。\n",
    "\n",
    "$K$ 回サイコロを振ったデータをまとめて $\\boldsymbol{\\mathsf{X}}=\\begin{pmatrix}\n",
    "\\boldsymbol{x}_1^\\top \\\\\n",
    "\\boldsymbol{x}_2^\\top \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}$ と書くことにすると、このモデルから $\\boldsymbol{\\mathsf{X}}$ が発生する同時確率は\n",
    "\n",
    "<span id=\"eq3\">\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{\\mathsf{X}}|\\boldsymbol{\\mu})=\\prod_{k=1}^K\\prod_{m=1}^6 \\mu_m^{x_{km}}\n",
    "\\end{equation}\n",
    "</span>\n",
    "\n",
    "この同時確率を $\\boldsymbol{\\mu}$ の関数とみなし、これを尤度関数という。よって対数尤度関数は\n",
    "<span id=\"eq4\">\n",
    "\\begin{equation}\n",
    "\\ln L(\\boldsymbol{\\mu})=\\ln P(\\boldsymbol{\\mathsf{X}}|\\boldsymbol{\\mu})=\\sum_{k=1}^K\\sum_{m=1}^6 x_{km}\\ln\\mu_m\n",
    "\\end{equation}\n",
    "</span>\n",
    "この関数を最大にするような $\\boldsymbol{\\mu}$ を求めればよい。\n",
    "\n",
    "ところで、$m$ の目が $c_m$ 回出たとすると、 $c_m=\\sum_{k=1}^K x_{km}$ だから\n",
    "<span id=\"eq5\">\n",
    "\\begin{equation}\n",
    "\\ln L(\\boldsymbol{\\mu})=\\sum_{m=1}^6 c_m\\ln\\mu_m\n",
    "\\end{equation}\n",
    "</span>\n",
    "と書いてもよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配法によるパラメータ推定\n",
    "尤度を最大にする $\\boldsymbol{\\mu}$ は解析的に求まり、\n",
    "<span id=\"eq-ml\">\n",
    "\\begin{equation}\n",
    "\\hat{\\mu}_m=\\frac{c_m}{K}\n",
    "\\end{equation}\n",
    "</span>\n",
    "である([課題](#kadai2))。わざわざパラメータ推定を勾配法で行う必要は全くないのだが、今回は練習のために敢えて勾配法で求めてみる。\n",
    "\n",
    "6通りの出目を持つサイコロを100回振った結果から、パラメータ(=出目の確率)を推定しよう。まず、$\\boldsymbol{\\mu}$ の真値を決めておく。全部等確率だとおもしろくないので、2の目の確率を0.5, そのほかを0.1とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 52,  7, 11,  8,  8])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu_true = [0.1, 0.5, 0.1, 0.1, 0.1, 0.1]\n",
    "c = np.random.multinomial(100, mu_true)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、[式(2)](#eq3)の確率分布にしたがって100回サイコロを振る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 6, 1, 2, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 2, 2, 4, 6, 6, 5, 5,\n",
       "       2, 6, 1, 3, 2, 2, 3, 2, 2, 5, 5, 1, 2, 2, 6, 5, 2, 6, 2, 2, 2, 2,\n",
       "       4, 6, 2, 2, 4, 6, 5, 2, 2, 4, 5, 5, 2, 4, 5, 2, 2, 3, 2, 2, 2, 6,\n",
       "       4, 2, 5, 2, 4, 2, 2, 2, 6, 3, 1, 6, 2, 3, 6, 6, 2, 6, 6, 2, 4, 2,\n",
       "       3, 2, 1, 2, 2, 6, 3, 2, 3, 4, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk = np.random.choice(6,100,p=mu_true)\n",
    "xk+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出目の回数は bincount 関数で数えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 46, 11,  9, 11, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.bincount(xk)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xkをone-hot表現に変換しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.identity(6)[xk]\n",
    "X[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[式(4)](#eq4)にしたがい、Xがデータとして与えられたときの対数尤度関数は次のように定義できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_likelihood(mu):\n",
    "    return np.sum(X * np.log(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hotベクトルでなく、出目の回数cがデータとして与えられた場合の対数尤度関数は、[式(5)](#eq5)より次のように定義できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_likelihood(mu):\n",
    "    return np.sum(c * np.log(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらが同じ結果を与えることを確認しておこう。適当な $\\mu$ に対して"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-205.69712487397635\n",
      "-205.69712487397632\n"
     ]
    }
   ],
   "source": [
    "mu = np.array([0.2,0.1,0.2,0.1,0.3,0.1])\n",
    "print(categorical_likelihood(mu))\n",
    "print(multinomial_likelihood(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ結果になった。以下ではデータとしてcを与えることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「ゼロから作るDeep Learning」 p.104\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1.0e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「ゼロから作るDeep Learning」 p.107\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.001, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は負の対数尤度で定義し、categorical_entropy_lossという名前を付けておく。\n",
    "\n",
    "ここで1つコツがある。 $\\mu_1,\\mu_2,\\cdots$ の総和は1でなければならないが、モデルパラメータにそのような制約を入れるのは少し厄介である。そこで、パラメータは和が1にならなくてもよいこととし、それらを $\\mu^\\text{raw}_{1}, \\mu^\\text{raw}_{2},\\cdots$ と書く(つまり $\\mu^\\text{raw}_{1}$ 等は確率ではない)。そして、確率は\n",
    "$$\\mu_m = \\frac{\\mu^\\text{raw}_{m}}{\\sum_{m=1}^6\\mu^\\text{raw}_{m}}$$\n",
    "のようにして計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_entropy_loss(mu_raw):\n",
    "    mu = mu_raw / np.sum(mu_raw)\n",
    "    return -multinomial_likelihood(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、パラメータを推定してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07      , 0.46000009, 0.10999999, 0.08999999, 0.10999999,\n",
       "       0.15999995])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_raw = gradient_descent(categorical_entropy_loss, np.random.rand(6),lr=0.01)\n",
    "mu = mu_raw / np.sum(mu_raw)\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 多クラスロジスティック回帰\n",
    "ロジスティック回帰を多クラス問題に拡張しよう。多クラスロジスティック回帰モデルは、線形予測子 $z_m$ および softmax 関数 $\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}$ により以下のように書くことができる ($m$ はクラス番号)。\n",
    "<span id=\"multiclasslogistic\">\n",
    "\\begin{align*}\n",
    "z_m&=\\boldsymbol{w_m}^\\top\\boldsymbol{x} = \\boldsymbol{x}^\\top\\boldsymbol{w_m}\\\\\n",
    "y_m&=\\text{softmax}(z_m)=\\frac{\\exp(z_m)}{\\sum_j \\exp(z_j)}\n",
    "\\end{align*}\n",
    "</span>\n",
    "\n",
    "ここで、$\\boldsymbol{w}_m$ はクラス $m$ の確率を求めるためのパラメータで、(説明変数の数+1)次元のベクトルである（線形回帰やロジスティック回帰と同じ）。このベクトルをクラス数分横に並べたのがパラメータ行列 $\\boldsymbol{W}=(\\boldsymbol{w}_1 \\boldsymbol{w}_2 \\cdots \\boldsymbol{w}_M)$ である。\n",
    "\n",
    "予測すべきクラスが $M$ 個あるので、線形予測子は $z_1, z_2, \\cdots z_M$ の $M$ 個ある。これらをひとまとめにすると\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{z}&=(z_1 z_2 \\cdots z_M) \\\\\n",
    "&=\\boldsymbol{x}^\\top\\boldsymbol{W}\n",
    "\\end{aligned}\n",
    "\n",
    "$k$番目のデータの説明変数を $\\boldsymbol{x}_k$, 目的変数を $\\boldsymbol{t}_k$ とする。ただし $\\boldsymbol{t}_k=(t_{k1} t_{k2} \\cdots t_{kM})^\\top$ は$k$番目のデータの正解クラスを表す one-hot 表現である。\n",
    "\n",
    "データセット $\\boldsymbol{\\mathsf{X}}=\\begin{pmatrix}\n",
    "\\boldsymbol{x}_1^\\top \\\\\n",
    "\\boldsymbol{x}_2^\\top \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}$,\n",
    "$\\boldsymbol{\\mathsf{T}}=\\begin{pmatrix}\n",
    "\\boldsymbol{t}_1^\\top \\\\\n",
    "\\boldsymbol{t}_2^\\top \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}$ に対し、 \n",
    "$k$番目のデータの線形予測子ベクトルは $\\boldsymbol{z}_k=\\boldsymbol{x}_k^\\top\\boldsymbol{W}$\n",
    "となる。よって、これを $K$ 個のデータ全部に対してまとめた行列 $\\boldsymbol{Z}$ は\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mathsf{Z}}=\\begin{pmatrix}\n",
    "\\boldsymbol{z}_1 \\\\\n",
    "\\boldsymbol{z}_2 \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\n",
    "=\\boldsymbol{X}^\\top\\boldsymbol{W}\n",
    "\\end{equation}\n",
    "なる $K\\times M$行列である。\n",
    "\n",
    "$k$番目のデータの線形予測子ベクトル $\\boldsymbol{z}_k$ から得られる各クラスの確率(予測値) をまとめて $\\boldsymbol{y}_k=(y_{k1} y_{k2} \\cdots y_{kM})$ で表す。\n",
    "$y_{km}=\\text{softmax}(z_{km})$ であるが、ここで $M$ 次元ベクトル $\\boldsymbol{z}_k$ を引数とし、 $M$ 個の予測値 $y_{k1} y_{k2} \\cdots y_{kM}$ からなるベクトル $\\boldsymbol{y}_k$ を返すベクトル版の softmax 関数をあらためて定義する。\n",
    "\\begin{equation}\n",
    "\\boldsymbol{y}_k=\\text{softmax}(\\boldsymbol{z}_k)\n",
    "\\end{equation}\n",
    "\n",
    "したがって、これを $K$ 個のデータ全部に対してまとめた行列\n",
    "$\\boldsymbol{\\mathsf{Y}}=\\begin{pmatrix}\n",
    "\\boldsymbol{y}_1 \\\\\n",
    "\\boldsymbol{y}_2 \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}$\n",
    "は、まず $\\boldsymbol{Z}=\\boldsymbol{X}^\\top\\boldsymbol{W}$\n",
    "を求め、次にその各行の softmax を求め、最後にそれを縦に並べて作ればよい。\n",
    "\n",
    "ロジスティック回帰で仮定した確率の[式(1)](#eq1)を[式(2)](#eq2)に置き換える。説明変数 $\\boldsymbol{x}_k$ によって多クラスロジスティック回帰モデルで予測されるクラス $\\boldsymbol{t}_k$ の確率は\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{t}_k|\\boldsymbol{w})=\\prod_m y_{km}^{t_{km}}\n",
    "\\end{equation}\n",
    "\n",
    "$\\boldsymbol{x}_1, \\boldsymbol{x}_2 \\cdots$ のそれぞれが $\\boldsymbol{t}_1, \\boldsymbol{t}_2 \\cdots$ になる同時確率は、それぞれの確率の積になるから\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{\\mathsf{T}}|\\boldsymbol{w})=\\prod_k \\prod_m y_{km}^{t_{km}}\n",
    "\\end{equation}\n",
    "\n",
    "よって、交差エントロピー（=負の対数尤度）は\n",
    "<span id=\"eq-ent\">\n",
    "\\begin{equation}\n",
    "E=-\\ln P(\\boldsymbol{\\mathsf{T}}|\\boldsymbol{w})=-\\sum_k \\sum_m t_{km} \\ln y_{km}\n",
    "\\end{equation}\n",
    "</span>\n",
    "となる。これは、行列 $\\boldsymbol{\\mathsf{T}}$ の各要素 $t_{km}$ と、行列 $\\boldsymbol{\\mathsf{Y}}$ の各要素 $y_{km}$ の対数の積を要素ごとに計算し、全ての要素に対して和を取る計算に相当する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris (アヤメの判別)\n",
    "ロジスティック回帰を用いてアヤメの花の寸法から3種類のアヤメを判別する問題を解いてみよう。irisデータセットについては各自調べておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "T = np.identity(3)[iris.target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "説明変数は4つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数tは3種類のアヤメの種類のone-hot表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]]),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0:5,:] , T[145:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0::2] # 偶数番目\n",
    "X_test = X[1::2] # 奇数番目\n",
    "T_train = T[0::2]\n",
    "T_test = T[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax関数を定義する。クラス数を $M$ とすると、引数は $M$ 次元のベクトル、戻り値は $M$ 次元のベクトルになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル版 softmax 関数\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この softmax 関数でも用は足りるが、データ全体を一挙に処理できる行列版の softmax 関数を作っておくと便利でしかも高速である。これは、行列の各行を1つのデータだとみなして softmax を実行し、結果の横ベクトルをデータ数だけ縦に並べた行列を返す関数である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行列版 softmax 関数\n",
    "def softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[式(11)](#eq-ent)に従って損失関数を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_loss(W):\n",
    "    Z = X_train_a.dot(W) # (KxD) x (DxM)= (KxM)\n",
    "    Y = softmax(Z)\n",
    "    return -np.sum(T_train * np.log(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ $\\boldsymbol{W}=(\\boldsymbol{w}_1 \\boldsymbol{w}_2 \\boldsymbol{w}_3)$ は今度は行列になるので、多次元配列に対応したバージョンの numerical_gradient を使う。([ここ](https://github.com/oreilly-japan/deep-learning-from-scratch)から入手できます)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a = np.column_stack((np.ones(X_train.shape[0]),X_train))\n",
    "X_test_a = np.column_stack((np.ones(X_test.shape[0]),X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=gradient_descent(categorical_cross_entropy_loss, np.random.rand(5,3), lr=0.001,step_num=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに対してロジスティック回帰モデルによる予測を行う。各データが各クラスの花である確率は次のように計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.74095230e-01, 2.59047659e-02, 3.96506782e-09],\n",
       "       [9.75138532e-01, 2.48614573e-02, 1.04707831e-08],\n",
       "       [9.93547039e-01, 6.45296026e-03, 2.76321659e-10],\n",
       "       [9.87897651e-01, 1.21023482e-02, 9.01165670e-10],\n",
       "       [9.75390348e-01, 2.46096496e-02, 2.88701765e-09],\n",
       "       [9.84761639e-01, 1.52383582e-02, 2.87125635e-09],\n",
       "       [9.89445617e-01, 1.05543806e-02, 1.94996174e-09],\n",
       "       [9.98919080e-01, 1.08092023e-03, 3.53659676e-12],\n",
       "       [9.92314101e-01, 7.68589833e-03, 3.77256535e-10],\n",
       "       [9.95444927e-01, 4.55507320e-03, 1.76323022e-10],\n",
       "       [9.93780102e-01, 6.21989715e-03, 4.50954043e-10],\n",
       "       [9.69307837e-01, 3.06921475e-02, 1.56338415e-08],\n",
       "       [9.56580582e-01, 4.34194078e-02, 1.03151762e-08],\n",
       "       [9.90410919e-01, 9.58908057e-03, 3.31748775e-10],\n",
       "       [9.74851664e-01, 2.51483266e-02, 9.37913269e-09],\n",
       "       [9.85886910e-01, 1.41130887e-02, 7.95834279e-10],\n",
       "       [9.98762744e-01, 1.23725608e-03, 3.14928354e-12],\n",
       "       [9.90615891e-01, 9.38410888e-03, 3.52149619e-10],\n",
       "       [9.94788738e-01, 5.21126201e-03, 1.56933924e-10],\n",
       "       [9.87735344e-01, 1.22646556e-02, 6.87852299e-10],\n",
       "       [8.86419644e-01, 1.13580123e-01, 2.33536500e-07],\n",
       "       [9.85230461e-01, 1.47695322e-02, 7.15962545e-09],\n",
       "       [9.73134274e-01, 2.68657181e-02, 8.37505484e-09],\n",
       "       [9.85110790e-01, 1.48892065e-02, 3.57769513e-09],\n",
       "       [9.87877075e-01, 1.21229239e-02, 7.67796645e-10],\n",
       "       [8.02132077e-03, 9.64909879e-01, 2.70687999e-02],\n",
       "       [3.12460856e-03, 8.65942181e-01, 1.30933210e-01],\n",
       "       [2.96150237e-03, 8.35960202e-01, 1.61078296e-01],\n",
       "       [3.49089991e-02, 9.50867359e-01, 1.42236415e-02],\n",
       "       [1.11803798e-02, 8.45042358e-01, 1.43777262e-01],\n",
       "       [1.07498244e-02, 9.34731642e-01, 5.45185339e-02],\n",
       "       [2.11735763e-03, 8.59839199e-01, 1.38043443e-01],\n",
       "       [8.26163244e-03, 9.85190532e-01, 6.54783524e-03],\n",
       "       [8.70438505e-03, 9.79967000e-01, 1.13286145e-02],\n",
       "       [8.38061324e-03, 9.69690352e-01, 2.19290346e-02],\n",
       "       [1.20793136e-02, 9.77972248e-01, 9.94843832e-03],\n",
       "       [1.93234833e-03, 9.21318358e-01, 7.67492932e-02],\n",
       "       [6.42743758e-03, 9.82981722e-01, 1.05908406e-02],\n",
       "       [9.22219366e-04, 7.95231194e-01, 2.03846587e-01],\n",
       "       [3.19466051e-02, 9.65984720e-01, 2.06867461e-03],\n",
       "       [1.16658714e-02, 9.77003604e-01, 1.13305245e-02],\n",
       "       [1.12668734e-04, 2.39471014e-01, 7.60416318e-01],\n",
       "       [1.27649656e-02, 9.05974905e-01, 8.12601289e-02],\n",
       "       [1.06996509e-03, 9.36641651e-01, 6.22883841e-02],\n",
       "       [5.50523255e-03, 9.02918137e-01, 9.15766303e-02],\n",
       "       [3.79087308e-03, 9.12616838e-01, 8.35922890e-02],\n",
       "       [2.67510237e-02, 9.60042154e-01, 1.32068220e-02],\n",
       "       [1.32060108e-02, 9.59465991e-01, 2.73279981e-02],\n",
       "       [7.06660279e-03, 9.74837563e-01, 1.80958342e-02],\n",
       "       [9.51820060e-03, 9.47489384e-01, 4.29924153e-02],\n",
       "       [1.95661354e-05, 4.71578449e-02, 9.52822589e-01],\n",
       "       [1.33332665e-05, 7.11480923e-02, 9.28838574e-01],\n",
       "       [3.50198384e-07, 2.70222189e-02, 9.72977431e-01],\n",
       "       [2.48478906e-06, 9.40224277e-02, 9.05975087e-01],\n",
       "       [6.25472127e-06, 3.15736882e-02, 9.68420057e-01],\n",
       "       [2.64175020e-05, 1.16254581e-01, 8.83719002e-01],\n",
       "       [7.01787830e-06, 2.28643690e-02, 9.77128613e-01],\n",
       "       [4.16704512e-05, 6.05637978e-02, 9.39394532e-01],\n",
       "       [5.83195705e-06, 7.63948328e-02, 9.23599335e-01],\n",
       "       [3.52281739e-05, 2.03527495e-01, 7.96437276e-01],\n",
       "       [3.90377860e-05, 4.40237606e-02, 9.55937202e-01],\n",
       "       [2.69497769e-04, 3.92155140e-01, 6.07575362e-01],\n",
       "       [3.80835882e-05, 2.96211119e-01, 7.03750797e-01],\n",
       "       [6.19826831e-04, 3.99694447e-01, 5.99685726e-01],\n",
       "       [8.19713583e-05, 5.77843443e-01, 4.22074586e-01],\n",
       "       [9.05473211e-05, 5.03662831e-01, 4.96246622e-01],\n",
       "       [3.58262014e-04, 5.79704574e-01, 4.19937164e-01],\n",
       "       [3.70536453e-06, 8.71939508e-02, 9.12802344e-01],\n",
       "       [7.24601077e-05, 1.78730500e-01, 8.21197039e-01],\n",
       "       [1.11065030e-04, 2.63094869e-01, 7.36794066e-01],\n",
       "       [2.58073103e-04, 3.09739268e-01, 6.90002659e-01],\n",
       "       [3.05540820e-06, 2.23801734e-02, 9.77616771e-01],\n",
       "       [6.43691773e-05, 1.26850193e-01, 8.73085438e-01],\n",
       "       [1.39236772e-04, 2.29265910e-01, 7.70594854e-01],\n",
       "       [1.46656255e-04, 1.55014100e-01, 8.44839244e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=softmax(X_test_a.dot(W))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{y}=(y_1 y_2 y_3)$ の中で確率が最大のクラスが認識結果となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.array([np.argmax(y) for y in Y])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解精度を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.equal(result, [np.argmax(t) for t in T_test])) / T_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='newton-cg',multi_class='multinomial')\n",
    "logreg.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.868623402058041"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(T_train * logreg.predict_log_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.069775925779655"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(T_train * np.log(softmax(X_train_a.dot(W))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題\n",
    "1. [softmax関数の式](#multiclasslogistic)で、クラスの数を2とするとロジスティック関数と同じになることを示せ。\n",
    "1. <span id=\"kadai2\"> [カテゴリカル分布の最尤推定式](#eq-ml)を証明せよ。</span>(ヒント: この問題は制約付き最大化問題になるのでLagrangeの未定乗数法を使うと解ける。)\n",
    "1. 上で未定義のまま残しておいた categorical_cross_entropy_loss 関数を完成させ、アヤメの判別を実行せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
